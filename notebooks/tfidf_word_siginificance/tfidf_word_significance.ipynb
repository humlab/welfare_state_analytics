{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Significance using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "root_folder = os.path.abspath(os.path.join(globals()['_dh'][-1], \"../..\"))\n",
    "\n",
    "corpus_folder = os.path.join(root_folder, \"output\")\n",
    "\n",
    "sys.path = [ root_folder ] + sys.path\n",
    "\n",
    "import ipywidgets\n",
    "from beakerx import *\n",
    "from beakerx.object import beakerx\n",
    "from IPython.display import display\n",
    "\n",
    "import westac.corpus.vectorized_corpus as vectorized_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previously vectorized corpus\n",
    "\n",
    "The corpus was created using the following settings:\n",
    " - Tokens were converted to lower case.\n",
    " - Only tokens that contains at least one alphanumeric character (isalnum).\n",
    " - Accents are ot removed (remove_accents)\n",
    " - Min token length 2 (min_len)\n",
    " - Max length not set (max_len)\n",
    " - Numerals are removed (keep_numerals, -N)\n",
    " - Symbols are removed (keep_symbols, -S)\n",
    "\n",
    "Use the `corpus_vectorizer` module to create a new corpus with different settings.\n",
    "\n",
    "The loaded corpus is processed in the following ways:\n",
    "\n",
    " - Exclude tokens having a total word count less than 10\n",
    " - Include at most 50000 most frequent words words.\n",
    "\n",
    "Compute a new TF-IDF weighted corpus\n",
    "\n",
    " - Group documents by year\n",
    " - Compute mean TF-IDF for each year\n",
    "\n",
    "Some references:\n",
    "\n",
    " - [scikit-learn TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)\n",
    " - [Spark MLlib TF-IDF](https://spark.apache.org/docs/2.2.0/ml-features.html#tf-idf)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v_corpus = vectorized_corpus.VectorizedCorpus\\\n",
    "    .load('SOU_1945-1989_NN+VB+JJ_lemma_L0_+N_+S', corpus_folder)\\\n",
    "    .slice_by_n_count(10)\\\n",
    "    .slice_by_n_top(500000)\n",
    "\n",
    "tf_idf_corpus = v_corpus\\\n",
    "    .tf_idf()\\\n",
    "    .group_by_year2(aggregate_function='mean', dtype=np.float)\n",
    "\n",
    "#normalized_corpus = v_corpus\\\n",
    "#    .normalize()\\\n",
    "#    .group_by_year2(aggregate_function='mean', dtype=np.float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import logging\n",
    "import ipywidgets\n",
    "\n",
    "from   IPython.display import display\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def display_top_terms(data):\n",
    "\n",
    "    if data is None:\n",
    "        logger.info('No data to display!')\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        { k: [ x[0] for x in v ] for k, v in data.items() }\n",
    "    )\n",
    "    \n",
    "    display(df)\n",
    "\n",
    "\n",
    "def compute_top_terms(x_corpus, x_documents, n_top, idx_groups=None):\n",
    "    \n",
    "    data = {\n",
    "        x['label']: x_corpus.get_top_n_words(n=n_top, indices=x['indices'])\n",
    "            for x in idx_groups\n",
    "    }\n",
    "    return data\n",
    "    \n",
    "def display_gui(x_corpus, x_documents):\n",
    "\n",
    "    lw = lambda w: ipywidgets.Layout(width=w)\n",
    "    \n",
    "    year_min, year_max = x_documents.year.min(), x_documents.year.max()\n",
    "    years = list(range(year_min, year_max + 1))\n",
    "    decades = [ 10 * decade for decade in range((year_min // 10), (year_max // 10) + 1) ]\n",
    "    lustrums = [ lustrum for lustrum in range(year_min - year_min % 5, year_max  - year_max % 5, 5) ]\n",
    "\n",
    "    groups = [\n",
    "        ('year', [ {\n",
    "            'label': str(year),\n",
    "            'indices': [ year - year_min ]\n",
    "        } for year in  years]),\n",
    "        ('decade', [ {\n",
    "            'label': str(decade),\n",
    "            'indices': [ year - year_min for year in range(decade, decade+10) if year_min <= year <= year_max ]\n",
    "        } for decade in decades]),\n",
    "        ('lustrum', [ {\n",
    "            'label': str(lustrum),\n",
    "            'indices': [ year - year_min for year in range(lustrum, lustrum+5) if year_min <= year <= year_max ]\n",
    "        } for lustrum in lustrums])\n",
    "    ]\n",
    "    \n",
    "    w_n_top = ipywidgets.IntSlider(description='#words', min=10, max=1000, value=100, tooltip='Number of words to compute')\n",
    "    w_compute = ipywidgets.Button(description='Compute', icon='', button_style='Success', layout=lw('120px'))\n",
    "    w_output = ipywidgets.Output() #layout={'border': '1px solid black'})\n",
    "    w_groups = ipywidgets.Dropdown(options=groups, value=groups[0][1], description='Groups:')\n",
    "    \n",
    "    boxes = ipywidgets.VBox([\n",
    "        ipywidgets.HBox([\n",
    "            w_n_top,\n",
    "            w_groups,\n",
    "            w_compute\n",
    "        ], layout=ipywidgets.Layout(align_items='flex-end')),\n",
    "        w_output\n",
    "    ])\n",
    "\n",
    "    display(boxes)\n",
    "\n",
    "    def compute_callback_handler(*_args):\n",
    "        w_output.clear_output()\n",
    "        with w_output:\n",
    "            try:\n",
    "\n",
    "                w_compute.disabled = True\n",
    "\n",
    "                data = compute_top_terms(x_corpus, x_documents, n_top=w_n_top.value, idx_groups=w_groups.value)\n",
    "\n",
    "                display_top_terms(data)\n",
    "\n",
    "            except Exception as ex:\n",
    "                logger.error(ex)\n",
    "            finally:\n",
    "                w_compute.disabled = False\n",
    "                \n",
    "    w_compute.on_click(compute_callback_handler)\n",
    "\n",
    "display_gui(tf_idf_corpus, tf_idf_corpus.document_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_word(x_corpus, word):\n",
    "    wv = x_corpus.get_word_vector(word)\n",
    "\n",
    "    df = pd.DataFrame({'count': wv, 'year': x_corpus.document_index.year }).set_index('year')\n",
    "    df.plot()\n",
    "\n",
    "plot_word(v_corpus, 'arbete')\n",
    "plot_word(tf_idf_corpus, 'arbete')\n",
    "plot_word(yearly_tf_idf_corpus, 'arbete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import numpy as np\n",
    "numpy.nansum(a, axis=None, dtype=None, out=None, keepdims=<no value>)[source]\n",
    "numpy.nanmean(a, axis=None, dtype=None, out=None, keepdims=<no value>)[source]\n",
    "docs=[\"the house had a tiny little mouse\",\n",
    "      \"the cat saw the mouse\",\n",
    "      \"the mouse ran away from the house\",\n",
    "      \"the cat finally ate the mouse\",\n",
    "      \"the end of the mouse story\"\n",
    "     ]\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "tfidf = tfidf_vectorizer_vectors.todense()\n",
    "# TFIDF of words not in the doc will be 0, so replace them with nan\n",
    "tfidf[tfidf == 0] = np.nan\n",
    "# Use nanmean of numpy which will ignore nan while calculating the mean\n",
    "means = np.nansum(tfidf, axis=0)\n",
    "# convert it into a dictionary for later lookup\n",
    "means = dict(zip(tfidf_vectorizer.get_feature_names(), means.tolist()[0]))\n",
    "\n",
    "tfidf = tfidf_vectorizer_vectors.todense()\n",
    "# Argsort the full TFIDF dense vector\n",
    "ordered = np.argsort(tfidf*-1)\n",
    "words = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "top_k = 5\n",
    "for i, doc in enumerate(docs):\n",
    "    result = { }\n",
    "    # Pick top_k from each argsorted matrix for each doc\n",
    "    for t in range(top_k):\n",
    "        # Pick the top k word, find its average tfidf from the\n",
    "        # precomputed dictionary using nanmean and save it to later use\n",
    "        result[words[ordered[i,t]]] = means[words[ordered[i,t]]]\n",
    "    print (result )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}