{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import unittest\n",
    "import utility\n",
    "import types\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3}\n",
      "(namespace(filename='document_2013_1.txt', year=2013), ['a', 'a', 'b', 'c', 'c', 'c', 'c', 'd'])\n",
      "(namespace(filename='document_2013_2.txt', year=2013), ['a', 'a', 'b', 'b', 'c', 'c', 'c'])\n",
      "(namespace(filename='document_2014_1.txt', year=2014), ['a', 'a', 'b', 'b', 'b', 'c', 'c'])\n",
      "(namespace(filename='document_2014_2.txt', year=2014), ['a', 'a', 'b', 'b', 'b', 'b', 'c', 'd'])\n"
     ]
    }
   ],
   "source": [
    "flatten = lambda l: [ x for ws in l for x in ws]\n",
    "mock_corpus_data = [\n",
    "    ('document_2013_1.txt', 2013, generate_document([(2,'a'), (1,'b'), (4,'c'), (1,'d')])),\n",
    "    ('document_2013_2.txt', 2013, generate_document([(2,'a'), (2,'b'), (3,'c'), (0,'d')])),\n",
    "    ('document_2014_1.txt', 2014, generate_document([(2,'a'), (3,'b'), (2,'c'), (0,'d')])),\n",
    "    ('document_2014_2.txt', 2014, generate_document([(2,'a'), (4,'b'), (1,'c'), (1,'d')]))\n",
    "]\n",
    "\n",
    "class MockedProcessedCorpus():\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.token_documents = data\n",
    "        self.vocabulary = self.create_vocabulary()\n",
    "        self.n_tokens = { f: }\n",
    "    def create_vocabulary(self):\n",
    "        return { w: i for i, w in enumerate(sorted(list(set(flatten([ x[2] for x in self.token_documents]))))) }\n",
    "        \n",
    "    def documents(self):\n",
    "\n",
    "        for filename, year, tokens in self.token_documents:\n",
    "            yield types.SimpleNamespace(filename=filename, year=year), tokens\n",
    "\n",
    "    def generate_document(self, words):\n",
    "        document =  flatten([ n * w for n, w in words])\n",
    "        return document\n",
    "\n",
    "corpus = MockedProcessedCorpus(mock_corpus_data)\n",
    "print(corpus.vocabulary)\n",
    "\n",
    "for d in corpus.documents():\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..............\n",
      "----------------------------------------------------------------------\n",
      "Ran 14 tests in 0.069s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f43c4cbc9b0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import utility\n",
    "\n",
    "class test_TextFilesReader(unittest.TestCase):\n",
    "    \n",
    "    def test_archive_filenames_when_filter_txt_returns_txt_files(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, pattern='*.txt')\n",
    "        self.assertEqual(5, len(reader.archive_filenames))\n",
    "\n",
    "    def test_archive_filenames_when_filter_md_returns_md_files(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, pattern='*.md')\n",
    "        self.assertEqual(1, len(reader.archive_filenames))\n",
    "\n",
    "    def test_archive_filenames_when_filter_function_txt_returns_txt_files(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        itemfilter = lambda _, x: x.endswith('txt')\n",
    "        reader = utility.TextFilesReader(filename, itemfilter=itemfilter)\n",
    "        self.assertEqual(5, len(reader.archive_filenames))\n",
    "\n",
    "    def test_get_file_when_default_returns_unmodified_content(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_01_test.txt'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=False, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Tre svarta ekar ur snön.\\r\\n\" + \\\n",
    "                   \"Så grova, men fingerfärdiga.\\r\\n\" + \\\n",
    "                   \"Ur deras väldiga flaskor\\r\\n\" + \\\n",
    "                   \"ska grönskan skumma i vår.\"\n",
    "        self.assertEqual(document_name, result[0])\n",
    "        self.assertEqual(expected, result[1])\n",
    "        \n",
    "    def test_can_get_file_when_compress_whitespace_is_true_strips_whitespaces(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_01_test.txt'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=True, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Tre svarta ekar ur snön. \" + \\\n",
    "                   \"Så grova, men fingerfärdiga. \" + \\\n",
    "                   \"Ur deras väldiga flaskor \" + \\\n",
    "                   \"ska grönskan skumma i vår.\"\n",
    "        self.assertEqual(document_name, result[0])\n",
    "        self.assertEqual(expected, result[1])\n",
    "\n",
    "    def test_get_file_when_dehyphen_is_trye_removes_hyphens(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_03_test.txt'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=True, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Nordlig storm. Det är den i den tid när rönnbärsklasar mognar. Vaken i mörkret hör man \" + \\\n",
    "                   \"stjärnbilderna stampa i sina spiltor \" + \\\n",
    "                   \"högt över trädet\"\n",
    "        self.assertEqual(document_name, result[0])\n",
    "        self.assertEqual(expected, result[1])\n",
    "        \n",
    "    def test_get_file_when_file_exists_and_extractor_specified_returns_content_and_metadat(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_03_test.txt'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Nordlig storm. Det är den i den tid när rönnbärsklasar mognar. Vaken i mörkret hör man \" + \\\n",
    "                   \"stjärnbilderna stampa i sina spiltor \" + \\\n",
    "                   \"högt över trädet\"\n",
    "        self.assertEqual(document_name, result[0].filename)\n",
    "        self.assertEqual(2019, result[0].year)\n",
    "        self.assertEqual(3, result[0].serial_no)\n",
    "        self.assertEqual(expected, result[1])\n",
    "        \n",
    "    def test_get_index_when_extractor_passed_returns_metadata(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        result = reader.metadata\n",
    "        expected = [\n",
    "            types.SimpleNamespace(filename='dikt_2019_01_test.txt', serial_no=1, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_02_test.txt', serial_no=2, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_03_test.txt', serial_no=3, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2020_01_test.txt', serial_no=1, year=2020),\n",
    "            types.SimpleNamespace(filename='dikt_2020_02_test.txt', serial_no=2, year=2020)]\n",
    "        \n",
    "        self.assertEqual(len(expected), len(result))\n",
    "        for i in range(0,len(expected)):\n",
    "            self.assertEqual(expected[i], result[i])\n",
    "\n",
    "class test_Utilities(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        pass\n",
    " \n",
    "    def test_dehypen(self):\n",
    "\n",
    "        text = 'absdef\\n'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual(text, result)\n",
    "\n",
    "        text = 'abs-def\\n'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual(text, result)\n",
    "        \n",
    "        text = 'abs - def\\n'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual(text, result)\n",
    "        \n",
    "        text = 'abs-\\ndef'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual('absdef\\n', result)\n",
    "        \n",
    "        text = 'abs- \\r\\n def'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual('absdef\\n', result)\n",
    "    \n",
    "    def test_compress_whitespaces(self):\n",
    "        \n",
    "        text = 'absdef\\n'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('absdef', result)\n",
    "\n",
    "        text = ' absdef \\n'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual( 'absdef', result)\n",
    "        \n",
    "        text = 'abs  def'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('abs def', result)\n",
    "        \n",
    "        text = 'abs\\n def'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('abs def', result)\n",
    "        \n",
    "        text = 'abs- \\r\\n def'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('abs- def', result)\n",
    "        \n",
    "class Test_ExtractMeta(unittest.TestCase):\n",
    " \n",
    "    def test_extract_metadata_when_valid_regexp_returns_metadata_values(self):\n",
    "        filename = 'SOU 1957_5 Namn.txt'\n",
    "        meta = utility.extract_metadata(filename, year=r\".{4}(\\d{4})\\_.*\", serial_no=\".{8}\\_(\\d+).*\")\n",
    "        self.assertEqual(5, meta.serial_no)\n",
    "        self.assertEqual(1957, meta.year)\n",
    "\n",
    "    def test_extract_metadata_when_invalid_regexp_returns_none(self):\n",
    "        filename = 'xyz.txt'\n",
    "        meta = utility.extract_metadata(filename, value=r\".{4}(\\d{4})\\_.*\")\n",
    "        self.assertEqual(None, meta.value)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".............\n",
      "----------------------------------------------------------------------\n",
      "Ran 13 tests in 0.072s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f43c4d0d0f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import text_corpus\n",
    "import nltk.tokenize\n",
    "\n",
    "class Test_CorpusTextStream(unittest.TestCase):\n",
    " \n",
    "    def test_next_document_when_new_corpus_returns_document(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=True, dehyphen=True)\n",
    "        corpus = text_corpus.CorpusTextStream(reader)\n",
    "        result = next(corpus.documents())\n",
    "        expected = \"Tre svarta ekar ur snön. \" + \\\n",
    "                   \"Så grova, men fingerfärdiga. \" + \\\n",
    "                   \"Ur deras väldiga flaskor \" + \\\n",
    "                   \"ska grönskan skumma i vår.\"\n",
    "        self.assertEqual(expected, result[1])\n",
    "\n",
    "    def test_get_index_when_extract_passed_returns_metadata(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        corpus = text_corpus.CorpusTextStream(reader)\n",
    "        result = corpus.get_index()\n",
    "        expected = [\n",
    "            types.SimpleNamespace(filename='dikt_2019_01_test.txt', serial_no=1, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_02_test.txt', serial_no=2, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_03_test.txt', serial_no=3, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2020_01_test.txt', serial_no=1, year=2020),\n",
    "            types.SimpleNamespace(filename='dikt_2020_02_test.txt', serial_no=2, year=2020)\n",
    "        ]\n",
    "        self.assertEqual(len(expected), len(result))\n",
    "        for i in range(0,len(expected)):\n",
    "            self.assertEqual(expected[i], result[i])\n",
    "            \n",
    "    def test_get_index_when_no_extract_passed_returns_none(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=None, compress_whitespaces=True, dehyphen=True)\n",
    "        corpus = text_corpus.CorpusTextStream(reader)\n",
    "        result = corpus.get_index()\n",
    "        self.assertIsNone(result)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........................\n",
      "----------------------------------------------------------------------\n",
      "Ran 26 tests in 0.231s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f43c3a86358>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test_CorpusTokenStream(unittest.TestCase):\n",
    "    \n",
    "    def create_reader(self, compress_whitespaces=True, dehyphen=True, meta_extract=None):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        #meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=compress_whitespaces, dehyphen=dehyphen)\n",
    "        return reader\n",
    "        \n",
    "    def test_next_document_when_token_corpus_returns_tokenized_document(self):\n",
    "        reader = reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=False)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\", \".\",\n",
    "                    \"Så\", \"grova\", \",\", \"men\", \"fingerfärdiga\", \".\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\", \".\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_isalnum_true_skips_deliminators(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=True)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"Så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_get_index_when_extract_passed_returns_expected_count(self):\n",
    "        reader = self.create_reader(meta_extract=dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\"))\n",
    "        corpus = text_corpus.CorpusTokenStream(reader)\n",
    "        result = corpus.get_index()\n",
    "        self.assertEqual(5, len(result))\n",
    "        \n",
    "    def test_n_tokens_when_exhausted_iterater_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=False)\n",
    "        r_n_tokens = {}\n",
    "        for filename, tokens in corpus.documents():\n",
    "            r_n_tokens[filename] = len(tokens)\n",
    "        n_tokens = corpus.n_tokens\n",
    "        expected = {\n",
    "            'dikt_2019_01_test.txt': 22,\n",
    "            'dikt_2019_02_test.txt': 16,\n",
    "            'dikt_2019_03_test.txt': 26,\n",
    "            'dikt_2020_01_test.txt': 45,\n",
    "            'dikt_2020_02_test.txt': 21\n",
    "        }\n",
    "        self.assertEqual(expected, n_tokens)\n",
    "        self.assertEqual(expected, r_n_tokens)\n",
    "\n",
    "    def test_n_tokens_when_exhausted_and_isalnum_is_true_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=True)\n",
    "        r_n_tokens = {}\n",
    "        for filename, tokens in corpus.documents():\n",
    "            r_n_tokens[filename] = len(tokens)\n",
    "        n_tokens = corpus.n_tokens\n",
    "        expected = {\n",
    "            'dikt_2019_01_test.txt': 18,\n",
    "            'dikt_2019_02_test.txt': 14,\n",
    "            'dikt_2019_03_test.txt': 24,\n",
    "            'dikt_2020_01_test.txt': 42,\n",
    "            'dikt_2020_02_test.txt': 18\n",
    "        }\n",
    "        self.assertEqual(expected, n_tokens)\n",
    "        self.assertEqual(expected, r_n_tokens)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........................\n",
      "----------------------------------------------------------------------\n",
      "Ran 26 tests in 0.220s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f43c309ff60>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test_ProcessedCorpus(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        pass\n",
    "    \n",
    "    def create_reader(self):\n",
    "        filename = '../data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        return reader\n",
    "    \n",
    "    def test_next_document_when_isalnum_is_true_returns_all_tokens(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=False, to_lower=False, deacc=False, min_len=1, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\", \".\",\n",
    "                    \"Så\", \"grova\", \",\", \"men\", \"fingerfärdiga\", \".\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\", \".\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_isalnum_true_skips_deliminators(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=False, deacc=False, min_len=1, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"Så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_to_lower_is_true_returns_all_lowercase(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=1, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_min_len_is_two_returns_single_char_words_filtered_out(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=2, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_max_len_is_six_returns_filter_out_longer_words(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=2, max_len=6, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"så\", \"grova\", \"men\", \n",
    "                    \"ur\", \"deras\", \n",
    "                    \"ska\", \"skumma\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_get_index_when_extract_passed_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=False, to_lower=False, deacc=False, min_len=2, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        result = corpus.get_index()\n",
    "        self.assertEqual(5, len(result))\n",
    "        \n",
    "    def test_n_tokens_when_exhausted_and_isalnum_min_len_two_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, isalnum=True, min_len=2)\n",
    "        r_tokens = {}\n",
    "        for filename, tokens in corpus.documents():\n",
    "            r_tokens[filename] = len(tokens)\n",
    "        n_tokens = corpus.n_raw_tokens\n",
    "        n_expected = {\n",
    "            'dikt_2019_01_test.txt': 18,\n",
    "            'dikt_2019_02_test.txt': 14,\n",
    "            'dikt_2019_03_test.txt': 24,\n",
    "            'dikt_2020_01_test.txt': 42,\n",
    "            'dikt_2020_02_test.txt': 18\n",
    "        }\n",
    "        p_tokens = corpus.n_tokens\n",
    "        p_expected = {\n",
    "            'dikt_2019_01_test.txt': 17,\n",
    "            'dikt_2019_02_test.txt': 13,\n",
    "            'dikt_2019_03_test.txt': 21,\n",
    "            'dikt_2020_01_test.txt': 42,\n",
    "            'dikt_2020_02_test.txt': 18\n",
    "        }\n",
    "        self.assertEqual(n_expected, n_tokens)\n",
    "        self.assertEqual(p_expected, p_tokens)\n",
    "        self.assertEqual(p_expected, r_tokens)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
