{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration #1: Create word frequency distribution over years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "sys.path = list(set(sys.path + [ '../common' ]))\n",
    "\n",
    "import corpus_vectorizer\n",
    "import text_corpus\n",
    "import utility\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(filename):\n",
    "    meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "    reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "    kwargs = dict(isalnum=False, to_lower=False, deacc=False, min_len=2, max_len=None, numerals=False)\n",
    "    corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "    return corpus\n",
    "\n",
    "def sum_by_year(X, df):\n",
    "    # df.groupby(['year']).apply(lambda x: x.index.tolist()).to_dict()\n",
    "    min_value, max_value = df.year.min(), df.year.max()\n",
    "\n",
    "    Y = np.zeros(((max_value - min_value) + 1, X.shape[1]))\n",
    "\n",
    "    for i in range(0, Y.shape[0]):\n",
    "\n",
    "        indices = list((df.loc[df.year == min_value + i].index))\n",
    "        if len(indices) > 0:\n",
    "            Y[i,:] = X[indices,:].sum(axis=0)\n",
    "\n",
    "    return Y\n",
    "\n",
    "def normalize(X):\n",
    "    Xn = sklearn.preprocessing.normalize(X, axis=1, norm='l1')\n",
    "    return Xn\n",
    "\n",
    "def tokens_above_threshold(vectorizer, threshold):\n",
    "    words = {\n",
    "        w: c for w,c in vectorizer.word_counts.items() if c >= threshold\n",
    "    }\n",
    "    return words\n",
    "\n",
    "def token_ids_above_threshold(vectorizer, threshold):\n",
    "    ids = [\n",
    "        vectorizer.vocabulary[w] for w in tokens_above_threshold(vectorizer, threshold).keys()\n",
    "    ]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "https://github.com/davidmcclure/lint-analysis/tree/master/notebooks/2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness-of-fit to uniform distribution (chi-square)\n",
    "\n",
    "See [scipy.stats.chisquare](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html): \n",
    "*When just f_obs is given, it is assumed that the expected frequencies are uniform and given by the mean of the observed frequencies.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "filename = './test/test_data/test_corpus.zip'\n",
    "#filename = './data/Sample_1945-1989_1.zip'\n",
    "#filename = './data/SOU_1945-1989.zip'\n",
    "\n",
    "vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "corpus = create_corpus(filename)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "dump_name = os.path.basename(filename).split('.')[0]\n",
    "vectorizer.dump(dump_name, folder='./output')\n",
    "\n",
    "if False:\n",
    "    Y = sum_by_year(X, vectorizer.document_index)\n",
    "\n",
    "    Yn = sklearn.preprocessing.normalize(Y, axis=1, norm='l1') \n",
    "\n",
    "    tokens_of_interest = list(tokens_above_threshold(vectorizer, 2).keys())\n",
    "    indices = token_ids_above_threshold(vectorizer, 2)\n",
    "\n",
    "    Ynw = Yn[:, indices]\n",
    "\n",
    "    stats.chisquare(Ynw, f_exp=None, ddof=0, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ward Clustering\n",
    "\n",
    "See [this](https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/) tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "linked = linkage(Z.todense(), 'ward')\n",
    "\n",
    "labelList = tokens_of_interest\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, orientation='top', labels=labelList, distance_sort='descending', show_leaf_counts=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find word with frequency > 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Zy.sum().where(lambda x: x>= 10000).sort_values().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to relative frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#Xn = normalize(X, axis=1, norm='l1')\n",
    "#Y = collapse_to_year_matrix(X, df_documents)\n",
    "#df = pd.DataFrame(Y, columns=list(vectorizer.get_feature_names()))\n",
    "#df.to_excel('test.xlsx')\n",
    "\n",
    "if False:\n",
    "    df = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names()))\n",
    "    df['year'] = df.index + 45\n",
    "    df = df.set_index('year')\n",
    "    df['year'] =  pd.Series(df.index).apply(lambda x: documents[x][0])\n",
    "    %matplotlib inline\n",
    "    df[['krig']].plot() #.loc[df[\"000\"]==49]\n",
    "\n",
    "Tara McPherson (Digital Literacy)\n",
    "Fikkers: Filmad presentation\n",
    "\n",
    "- searching-what do algorthms do\n",
    "- Documentation\n",
    "- Analysis - how do they work, what are their limitations, ...\n",
    "- Presentation  - recontextualization\n",
    "- Narration\n",
    "\n",
    "new kind of crithisism - not only focues on source crithisism - digital critisism"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
