{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import unittest\n",
    "import utility\n",
    "import types\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3}\n",
      "(namespace(filename='document_2013_1.txt', year=2013), ['a', 'a', 'b', 'c', 'c', 'c', 'c', 'd'])\n",
      "(namespace(filename='document_2013_2.txt', year=2013), ['a', 'a', 'b', 'b', 'c', 'c', 'c'])\n",
      "(namespace(filename='document_2014_1.txt', year=2014), ['a', 'a', 'b', 'b', 'b', 'c', 'c'])\n",
      "(namespace(filename='document_2014_2.txt', year=2014), ['a', 'a', 'b', 'b', 'b', 'b', 'c', 'd'])\n"
     ]
    }
   ],
   "source": [
    "flatten = lambda l: [ x for ws in l for x in ws]\n",
    "\n",
    "class MockedProcessedCorpus():\n",
    "\n",
    "    def __init__(self, mock_data):\n",
    "        self.tokenized_documents = [ (f,y,self.generate_document(ws)) for f,y,ws in mock_data]\n",
    "        self.vocabulary = self.create_vocabulary()\n",
    "        self.n_tokens = { f: len(d) for f,y,d in mock_corpus_data }\n",
    "        \n",
    "    def create_vocabulary(self):\n",
    "        return { w: i for i, w in enumerate(sorted(list(set(flatten([ x[2] for x in self.tokenized_documents]))))) }\n",
    "        \n",
    "    def documents(self):\n",
    "\n",
    "        for filename, year, tokens in self.tokenized_documents:\n",
    "            yield types.SimpleNamespace(filename=filename, year=year), tokens\n",
    "\n",
    "    def generate_document(self, words):\n",
    "        document =  flatten([ n * w for n, w in words])\n",
    "        return document\n",
    "\n",
    "mock_corpus_data = [\n",
    "    ('document_2013_1.txt', 2013, [(2,'a'), (1,'b'), (4,'c'), (1,'d')]),\n",
    "    ('document_2013_2.txt', 2013, [(2,'a'), (2,'b'), (3,'c'), (0,'d')]),\n",
    "    ('document_2014_1.txt', 2014, [(2,'a'), (3,'b'), (2,'c'), (0,'d')]),\n",
    "    ('document_2014_2.txt', 2014, [(2,'a'), (4,'b'), (1,'c'), (1,'d')])\n",
    "]\n",
    "\n",
    "corpus = MockedProcessedCorpus(mock_corpus_data)\n",
    "print(corpus.vocabulary)\n",
    "\n",
    "for d in corpus.documents():\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EE............\n",
      "======================================================================\n",
      "ERROR: test_fit_transform_ (__main__.Test_CorpusVectorizer)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-8223601fd039>\", line 19, in test_fit_transform_\n",
      "    vectorizer = CorpusVectorizer()\n",
      "NameError: name 'CorpusVectorizer' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_word_counts (__main__.Test_CorpusVectorizer)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-8223601fd039>\", line 25, in test_word_counts\n",
      "    vectorizer = CorpusVectorizer()\n",
      "NameError: name 'CorpusVectorizer' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 14 tests in 0.057s\n",
      "\n",
      "FAILED (errors=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fd685007b38>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import utility\n",
    "\n",
    "class test_TextFilesReader(unittest.TestCase):\n",
    "    \n",
    "    def test_archive_filenames_when_filter_txt_returns_txt_files(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, pattern='*.txt')\n",
    "        self.assertEqual(5, len(reader.archive_filenames))\n",
    "\n",
    "    def test_archive_filenames_when_filter_md_returns_md_files(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, pattern='*.md')\n",
    "        self.assertEqual(1, len(reader.archive_filenames))\n",
    "\n",
    "    def test_archive_filenames_when_filter_function_txt_returns_txt_files(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        itemfilter = lambda _, x: x.endswith('txt')\n",
    "        reader = utility.TextFilesReader(filename, itemfilter=itemfilter)\n",
    "        self.assertEqual(5, len(reader.archive_filenames))\n",
    "\n",
    "    def test_get_file_when_default_returns_unmodified_content(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_01_test.txt'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=False, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Tre svarta ekar ur snön.\\r\\n\" + \\\n",
    "                   \"Så grova, men fingerfärdiga.\\r\\n\" + \\\n",
    "                   \"Ur deras väldiga flaskor\\r\\n\" + \\\n",
    "                   \"ska grönskan skumma i vår.\"\n",
    "        self.assertEqual(document_name, result[0])\n",
    "        self.assertEqual(expected, result[1])\n",
    "        \n",
    "    def test_can_get_file_when_compress_whitespace_is_true_strips_whitespaces(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_01_test.txt'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=True, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Tre svarta ekar ur snön. \" + \\\n",
    "                   \"Så grova, men fingerfärdiga. \" + \\\n",
    "                   \"Ur deras väldiga flaskor \" + \\\n",
    "                   \"ska grönskan skumma i vår.\"\n",
    "        self.assertEqual(document_name, result[0])\n",
    "        self.assertEqual(expected, result[1])\n",
    "\n",
    "    def test_get_file_when_dehyphen_is_trye_removes_hyphens(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_03_test.txt'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=True, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Nordlig storm. Det är den i den tid när rönnbärsklasar mognar. Vaken i mörkret hör man \" + \\\n",
    "                   \"stjärnbilderna stampa i sina spiltor \" + \\\n",
    "                   \"högt över trädet\"\n",
    "        self.assertEqual(document_name, result[0])\n",
    "        self.assertEqual(expected, result[1])\n",
    "        \n",
    "    def test_get_file_when_file_exists_and_extractor_specified_returns_content_and_metadat(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_03_test.txt'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Nordlig storm. Det är den i den tid när rönnbärsklasar mognar. Vaken i mörkret hör man \" + \\\n",
    "                   \"stjärnbilderna stampa i sina spiltor \" + \\\n",
    "                   \"högt över trädet\"\n",
    "        self.assertEqual(document_name, result[0].filename)\n",
    "        self.assertEqual(2019, result[0].year)\n",
    "        self.assertEqual(3, result[0].serial_no)\n",
    "        self.assertEqual(expected, result[1])\n",
    "        \n",
    "    def test_get_index_when_extractor_passed_returns_metadata(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        result = reader.metadata\n",
    "        expected = [\n",
    "            types.SimpleNamespace(filename='dikt_2019_01_test.txt', serial_no=1, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_02_test.txt', serial_no=2, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_03_test.txt', serial_no=3, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2020_01_test.txt', serial_no=1, year=2020),\n",
    "            types.SimpleNamespace(filename='dikt_2020_02_test.txt', serial_no=2, year=2020)]\n",
    "        \n",
    "        self.assertEqual(len(expected), len(result))\n",
    "        for i in range(0,len(expected)):\n",
    "            self.assertEqual(expected[i], result[i])\n",
    "\n",
    "class test_Utilities(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        pass\n",
    " \n",
    "    def test_dehypen(self):\n",
    "\n",
    "        text = 'absdef\\n'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual(text, result)\n",
    "\n",
    "        text = 'abs-def\\n'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual(text, result)\n",
    "        \n",
    "        text = 'abs - def\\n'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual(text, result)\n",
    "        \n",
    "        text = 'abs-\\ndef'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual('absdef\\n', result)\n",
    "        \n",
    "        text = 'abs- \\r\\n def'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual('absdef\\n', result)\n",
    "    \n",
    "    def test_compress_whitespaces(self):\n",
    "        \n",
    "        text = 'absdef\\n'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('absdef', result)\n",
    "\n",
    "        text = ' absdef \\n'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual( 'absdef', result)\n",
    "        \n",
    "        text = 'abs  def'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('abs def', result)\n",
    "        \n",
    "        text = 'abs\\n def'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('abs def', result)\n",
    "        \n",
    "        text = 'abs- \\r\\n def'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('abs- def', result)\n",
    "        \n",
    "class Test_ExtractMeta(unittest.TestCase):\n",
    " \n",
    "    def test_extract_metadata_when_valid_regexp_returns_metadata_values(self):\n",
    "        filename = 'SOU 1957_5 Namn.txt'\n",
    "        meta = utility.extract_metadata(filename, year=r\".{4}(\\d{4})\\_.*\", serial_no=\".{8}\\_(\\d+).*\")\n",
    "        self.assertEqual(5, meta.serial_no)\n",
    "        self.assertEqual(1957, meta.year)\n",
    "\n",
    "    def test_extract_metadata_when_invalid_regexp_returns_none(self):\n",
    "        filename = 'xyz.txt'\n",
    "        meta = utility.extract_metadata(filename, value=r\".{4}(\\d{4})\\_.*\")\n",
    "        self.assertEqual(None, meta.value)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".............\n",
      "----------------------------------------------------------------------\n",
      "Ran 13 tests in 0.072s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f43c4d0d0f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import text_corpus\n",
    "import nltk.tokenize\n",
    "\n",
    "class Test_CorpusTextStream(unittest.TestCase):\n",
    " \n",
    "    def test_next_document_when_new_corpus_returns_document(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=True, dehyphen=True)\n",
    "        corpus = text_corpus.CorpusTextStream(reader)\n",
    "        result = next(corpus.documents())\n",
    "        expected = \"Tre svarta ekar ur snön. \" + \\\n",
    "                   \"Så grova, men fingerfärdiga. \" + \\\n",
    "                   \"Ur deras väldiga flaskor \" + \\\n",
    "                   \"ska grönskan skumma i vår.\"\n",
    "        self.assertEqual(expected, result[1])\n",
    "\n",
    "    def test_get_index_when_extract_passed_returns_metadata(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        corpus = text_corpus.CorpusTextStream(reader)\n",
    "        result = corpus.get_index()\n",
    "        expected = [\n",
    "            types.SimpleNamespace(filename='dikt_2019_01_test.txt', serial_no=1, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_02_test.txt', serial_no=2, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_03_test.txt', serial_no=3, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2020_01_test.txt', serial_no=1, year=2020),\n",
    "            types.SimpleNamespace(filename='dikt_2020_02_test.txt', serial_no=2, year=2020)\n",
    "        ]\n",
    "        self.assertEqual(len(expected), len(result))\n",
    "        for i in range(0,len(expected)):\n",
    "            self.assertEqual(expected[i], result[i])\n",
    "            \n",
    "    def test_get_index_when_no_extract_passed_returns_none(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=None, compress_whitespaces=True, dehyphen=True)\n",
    "        corpus = text_corpus.CorpusTextStream(reader)\n",
    "        result = corpus.get_index()\n",
    "        self.assertIsNone(result)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........................\n",
      "----------------------------------------------------------------------\n",
      "Ran 26 tests in 0.231s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f43c3a86358>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test_CorpusTokenStream(unittest.TestCase):\n",
    "    \n",
    "    def create_reader(self, compress_whitespaces=True, dehyphen=True, meta_extract=None):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        #meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=compress_whitespaces, dehyphen=dehyphen)\n",
    "        return reader\n",
    "        \n",
    "    def test_next_document_when_token_corpus_returns_tokenized_document(self):\n",
    "        reader = reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=False)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\", \".\",\n",
    "                    \"Så\", \"grova\", \",\", \"men\", \"fingerfärdiga\", \".\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\", \".\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_isalnum_true_skips_deliminators(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=True)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"Så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_get_index_when_extract_passed_returns_expected_count(self):\n",
    "        reader = self.create_reader(meta_extract=dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\"))\n",
    "        corpus = text_corpus.CorpusTokenStream(reader)\n",
    "        result = corpus.get_index()\n",
    "        self.assertEqual(5, len(result))\n",
    "        \n",
    "    def test_n_tokens_when_exhausted_iterater_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=False)\n",
    "        r_n_tokens = {}\n",
    "        for filename, tokens in corpus.documents():\n",
    "            r_n_tokens[filename] = len(tokens)\n",
    "        n_tokens = corpus.n_tokens\n",
    "        expected = {\n",
    "            'dikt_2019_01_test.txt': 22,\n",
    "            'dikt_2019_02_test.txt': 16,\n",
    "            'dikt_2019_03_test.txt': 26,\n",
    "            'dikt_2020_01_test.txt': 45,\n",
    "            'dikt_2020_02_test.txt': 21\n",
    "        }\n",
    "        self.assertEqual(expected, n_tokens)\n",
    "        self.assertEqual(expected, r_n_tokens)\n",
    "\n",
    "    def test_n_tokens_when_exhausted_and_isalnum_is_true_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=True)\n",
    "        r_n_tokens = {}\n",
    "        for filename, tokens in corpus.documents():\n",
    "            r_n_tokens[filename] = len(tokens)\n",
    "        n_tokens = corpus.n_tokens\n",
    "        expected = {\n",
    "            'dikt_2019_01_test.txt': 18,\n",
    "            'dikt_2019_02_test.txt': 14,\n",
    "            'dikt_2019_03_test.txt': 24,\n",
    "            'dikt_2020_01_test.txt': 42,\n",
    "            'dikt_2020_02_test.txt': 18\n",
    "        }\n",
    "        self.assertEqual(expected, n_tokens)\n",
    "        self.assertEqual(expected, r_n_tokens)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........................\n",
      "----------------------------------------------------------------------\n",
      "Ran 26 tests in 0.220s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f43c309ff60>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test_ProcessedCorpus(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        pass\n",
    "    \n",
    "    def create_reader(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        return reader\n",
    "    \n",
    "    def test_next_document_when_isalnum_is_true_returns_all_tokens(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=False, to_lower=False, deacc=False, min_len=1, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\", \".\",\n",
    "                    \"Så\", \"grova\", \",\", \"men\", \"fingerfärdiga\", \".\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\", \".\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_isalnum_true_skips_deliminators(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=False, deacc=False, min_len=1, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"Så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_to_lower_is_true_returns_all_lowercase(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=1, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_min_len_is_two_returns_single_char_words_filtered_out(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=2, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_max_len_is_six_returns_filter_out_longer_words(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=2, max_len=6, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"så\", \"grova\", \"men\", \n",
    "                    \"ur\", \"deras\", \n",
    "                    \"ska\", \"skumma\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_get_index_when_extract_passed_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=False, to_lower=False, deacc=False, min_len=2, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        result = corpus.get_index()\n",
    "        self.assertEqual(5, len(result))\n",
    "        \n",
    "    def test_n_tokens_when_exhausted_and_isalnum_min_len_two_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, isalnum=True, min_len=2)\n",
    "        r_tokens = {}\n",
    "        for filename, tokens in corpus.documents():\n",
    "            r_tokens[filename] = len(tokens)\n",
    "        n_tokens = corpus.n_raw_tokens\n",
    "        n_expected = {\n",
    "            'dikt_2019_01_test.txt': 18,\n",
    "            'dikt_2019_02_test.txt': 14,\n",
    "            'dikt_2019_03_test.txt': 24,\n",
    "            'dikt_2020_01_test.txt': 42,\n",
    "            'dikt_2020_02_test.txt': 18\n",
    "        }\n",
    "        p_tokens = corpus.n_tokens\n",
    "        p_expected = {\n",
    "            'dikt_2019_01_test.txt': 17,\n",
    "            'dikt_2019_02_test.txt': 13,\n",
    "            'dikt_2019_03_test.txt': 21,\n",
    "            'dikt_2020_01_test.txt': 42,\n",
    "            'dikt_2020_02_test.txt': 18\n",
    "        }\n",
    "        self.assertEqual(n_expected, n_tokens)\n",
    "        self.assertEqual(p_expected, p_tokens)\n",
    "        self.assertEqual(p_expected, r_tokens)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...............\n",
      "----------------------------------------------------------------------\n",
      "Ran 15 tests in 0.164s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fd67ceb9f98>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import text_corpus\n",
    "import corpus_vectorizer\n",
    "import os\n",
    "\n",
    "class Test_CorpusVectorizer(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        pass\n",
    "    \n",
    "    def create_corpus(self):\n",
    "        filename = '../test/test_data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=2, max_len=None, numerals=False)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        return corpus\n",
    "    \n",
    "    def test_fit_transform_(self):\n",
    "        corpus = self.create_corpus()\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        vectorizer.fit_transform(corpus)\n",
    "        results = vectorizer.vocabulary\n",
    "        expected = {'tre': 69, 'svarta': 62, 'ekar': 9, 'ur': 72, 'snön': 54, 'så': 65, 'grova': 17, 'men': 32, 'fingerfärdiga': 13, 'deras': 6, 'väldiga': 78, 'flaskor': 14, 'ska': 50, 'grönskan': 19, 'skumma': 53, 'vår': 79, 'på': 44, 'väg': 77, 'det': 7, 'långa': 29, 'mörkret': 36, 'envist': 11, 'skimrar': 51, 'mitt': 33, 'armbandsur': 2, 'med': 31, 'tidens': 67, 'fångna': 16, 'insekt': 25, 'nordlig': 38, 'storm': 61, 'är': 81, 'den': 5, 'tid': 66, 'när': 39, 'rönnbärsklasar': 45, 'mognar': 34, 'vaken': 74, 'hör': 24, 'man': 30, 'stjärnbilderna': 59, 'stampa': 58, 'sina': 48, 'spiltor': 57, 'högt': 23, 'över': 82, 'trädet': 70, 'jag': 26, 'ligger': 28, 'sängen': 64, 'armarna': 1, 'utbredda': 73, 'ett': 12, 'ankare': 0, 'som': 55, 'grävt': 18, 'ner': 37, 'sig': 47, 'ordentligt': 42, 'och': 40, 'håller': 22, 'kvar': 27, 'skuggan': 52, 'flyter': 15, 'där': 8, 'ovan': 43, 'stora': 60, 'okända': 41, 'en': 10, 'del': 4, 'av': 3, 'säkert': 63, 'viktigare': 76, 'än': 80, 'har': 20, 'sett': 46, 'mycket': 35, 'verkligheten': 75, 'tärt': 71, 'här': 21, 'sommaren': 56, 'till': 68, 'sist': 49}\n",
    "        self.assertEqual(expected, results)\n",
    "        \n",
    "    def test_word_counts(self):\n",
    "        corpus = self.create_corpus()\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "        results = vectorizer.word_counts\n",
    "        expected = {\n",
    "            'tre': 1, 'svarta': 1, 'ekar': 1, 'ur': 2, 'snön': 1, 'så': 3, 'grova': 1, 'men': 2, 'fingerfärdiga': 1,\n",
    "            'deras': 1, 'väldiga': 2, 'flaskor': 1, 'ska': 1, 'grönskan': 1, 'skumma': 1, 'vår': 1, 'på': 3, 'väg': 1,\n",
    "            'det': 3, 'långa': 1, 'mörkret': 2, 'envist': 1, 'skimrar': 1, 'mitt': 1, 'armbandsur': 1, 'med': 2, 'tidens': 1,\n",
    "            'fångna': 1, 'insekt': 1, 'nordlig': 1, 'storm': 1, 'är': 5, 'den': 3, 'tid': 1, 'när': 1, 'rönnbärsklasar': 1,\n",
    "            'mognar': 1, 'vaken': 1, 'hör': 1, 'man': 2, 'stjärnbilderna': 1, 'stampa': 1, 'sina': 1, 'spiltor': 1,\n",
    "            'högt': 1, 'över': 1, 'trädet': 1, 'jag': 4, 'ligger': 1, 'sängen': 1, 'armarna': 1, 'utbredda': 1, 'ett': 1,\n",
    "            'ankare': 1, 'som': 4, 'grävt': 1, 'ner': 1, 'sig': 1, 'ordentligt': 1, 'och': 2, 'håller': 1, 'kvar': 1,\n",
    "            'skuggan': 1, 'flyter': 1, 'där': 1, 'ovan': 1, 'stora': 1, 'okända': 1, 'en': 2, 'del': 1, 'av': 1, 'säkert': 1,\n",
    "            'viktigare': 1, 'än': 1, 'har': 2, 'sett': 1, 'mycket': 2, 'verkligheten': 1, 'tärt': 1, 'här': 1, 'sommaren': 1,\n",
    "            'till': 1, 'sist': 1\n",
    "        }\n",
    "        self.assertEqual(expected, results)\n",
    "        \n",
    "    def test_dump_can_be_loaded(self):\n",
    "\n",
    "        # Arrange\n",
    "        corpus = self.create_corpus()\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "        # Act\n",
    "        vectorizer.dump('dump_test', folder='./output')\n",
    "\n",
    "        # Assert\n",
    "        data_filename =\"./output/dump_test_vectorizer_data.pickle\"\n",
    "        matrix_filename = \"./output/dump_test_vector_data.npy\"\n",
    "        \n",
    "        self.assertTrue(os.path.isfile(data_filename))\n",
    "        self.assertTrue(os.path.isfile(matrix_filename))\n",
    "        \n",
    "        # Act\n",
    "        loaded_vectorizer = corpus_vectorizer.CorpusVectorizer().load('dump_test', folder='./output')\n",
    "        \n",
    "        # Assert\n",
    "        self.assertEqual(vectorizer.word_counts, loaded_vectorizer.word_counts)\n",
    "        self.assertEqual(vectorizer.document_index.to_dict(), loaded_vectorizer.document_index.to_dict())\n",
    "        self.assertEqual(vectorizer.vocabulary, loaded_vectorizer.vocabulary)\n",
    "        #self.assertEqual(vectorizer.X, loaded_vectorizer.X)\n",
    "        self.assertIsNone(loaded_vectorizer.corpus)\n",
    "        #self.assertEqual(vectorizer.vectorizer, loaded_vectorizer.vectorizer)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
