{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find word's that changes most over time\n",
    "\n",
    "## Approach\n",
    "\n",
    "Compare word's distribution over time with a uniform distribution. Use as null hypothesis the belief that a word's distribution does not change over time. Filter out all the words for which there is no significance.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Goodness_of_fit#Categorical_data\n",
    "\n",
    "\n",
    "## Candidate methods\n",
    "\n",
    "### Chi-square test for a discrete uniform distribution\n",
    "\n",
    "A χ2 goodness-of-fit test is used to determine how (un)likely a data serie (i.e. the word's distribution over time) has been generate by a (discrete) uniform distribution. The actual word counts for each year are used since χ2 is not applicable to relative frequencies. As a rule of thumb, χ2 test requires each individual value to be greater or equal to 5.\n",
    "\n",
    "From [stats.stackexchange.com](stats.stackexchange.com/questions/25827/how-does-one-measure-the-non-uniformity-of-a-distribution):\n",
    "\n",
    "*If you have not only the frequencies but the actual counts, you can use a χ2 goodness-of-fit test for each data series. In particular, you wish to use the test for a discrete uniform distribution. This gives you a good test, which allows you to find out which data series are likely not to have been generated by a uniform distribution, but does not provide a measure of uniformity..... (I guess that the chi-squared statistic can be seen as a measure of uniformity, but it has some drawbacks, such as the lack of convergence, dependence on the arbitrarily placed bins, that the number of expected counts in the cells needs to be sufficiently large, etc. Which measure/test to use is a matter of taste though, and entropy is not without its problems either (in particular, there are many different estimators of the entropy of a distribution). To me, entropy seems like a less arbitrary measure and is easier to interpret.)*\n",
    "\n",
    " $\\tilde{\\chi}^2=\\frac{1}{d}\\sum_{k=1}^{n} \\frac{(O_k - E_k)^2}{E_k}$ (d degree of freedom, n samples, E expected, O observed)\n",
    " \n",
    "References:\n",
    "\n",
    "  - [Chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test)\n",
    "  - [comparing-two-word-distributions](stats.stackexchange.com/questions/236192/comparing-two-word-distributions)\n",
    "\n",
    "### Simple linear regression\n",
    "\n",
    "Use least-squares fit to compute a Compare word's distribution over time with a uniform distribution. Use as null hypothesis the belief that a word's distribution does not change over time. Filter out all the words for which there is no significance.\n",
    "\n",
    "| Slope | $y = k * x + m$ | Use linear regression to compute slope k. Select n word having highest absoulute value |\n",
    "\n",
    "### G-test for a discrete uniform distribution\n",
    "\n",
    "en.wikipedia.org/wiki/G-test\n",
    "\n",
    "### Kolmogorov-Smirnov test (KS-test)\n",
    "\n",
    "stackoverflow.com/questions/25208421/how-to-test-for-uniformity\n",
    "\n",
    "### Entropy \n",
    "\n",
    "https://stats.stackexchange.com/questions/25827/how-does-one-measure-the-non-uniformity-of-a-distribution\n",
    "\n",
    "*There are other possible approaches, such as computing the entropy of each series - the uniform distribution maximizes the entropy, so if the entropy is suspiciously low you would conclude that you probably don't have a uniform distribution. That works as a measure of uniformity in some sense.*\n",
    "\n",
    "### Kullback-Leibler divergence (KS-test)\n",
    "Another suggestion would be to use a measure like the Kullback-Leibler divergence, which measures the similarity of two distributions.\n",
    "\n",
    "\n",
    "### L2 norm\n",
    "\n",
    "*Here is a simple heuristic: if you assume elements in any vector sum to 1 (or simply normalize each element with the sum to achieve this), then uniformity can be represented by L2 norm, which ranges from 1d√ to 1, with d being the dimension of vectors.\n",
    "\n",
    "The lower bound 1d√ corresponds to uniformity and upper bound to the 1-hot vector.\n",
    "\n",
    "To scale this to a score between 0 and 1, you can use n∗d√−1d√−1, where n is the L2 norm.\n",
    "\n",
    "An example modified from yours with elements summing to 1 and all vectors with the same dimension for simplicity:*\n",
    "\n",
    "0.10    0.11    0.10    0.09    0.09    0.11    0.10    0.10    0.12    0.08\n",
    "0.10    0.10    0.10    0.08    0.12    0.12    0.09    0.09    0.12    0.08\n",
    "0.03    0.02    0.61    0.02    0.03    0.07    0.06    0.05    0.06    0.05\n",
    "The following will yield 0.0028, 0.0051, and 0.4529 for the rows:\n",
    "\n",
    "```\n",
    "d = size(m,2); \n",
    "for i = 1 : size(m); \n",
    "    disp( ( norm(m(i,:))*sqrt(d)-1) / (sqrt(d)-1) ); \n",
    "end\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import operator\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from westac.common import vectorized_corpus\n",
    "\n",
    "import westac.common.utility as utility\n",
    "\n",
    "logger = utility.setup_logger(filename='./westac.log')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 4.8] \n",
    "pd.set_option('display.max_rows', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.0028\n0.0051\n0.4529\n"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "m = np.array([\n",
    "    [ 0.10, 0.11, 0.10, 0.09, 0.09, 0.11, 0.10, 0.10, 0.12, 0.08 ],\n",
    "    [ 0.10, 0.10, 0.10, 0.08, 0.12, 0.12, 0.09, 0.09, 0.12, 0.08 ],\n",
    "    [ 0.03, 0.02, 0.61, 0.02, 0.03, 0.07, 0.06, 0.05, 0.06, 0.05 ]\n",
    "])\n",
    "\n",
    "# The following will yield 0.0028, 0.0051, and 0.4529 for the rows:\n",
    "\n",
    "d = m.shape[1]\n",
    "\n",
    "for i in range(0, m.shape[0]):\n",
    "\n",
    "    l2_norm = (np.linalg.norm(m[i, :]) * math.sqrt(d) - 1 ) / (math.sqrt(d) - 1)\n",
    "\n",
    "    print('{:.4f}'.format(l2_norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0.0028 0.0051 0.4529]\n"
    }
   ],
   "source": [
    "def gof_by_l2_norm(matrix, axis=1):\n",
    "\n",
    "    \"\"\" Computes L2 norm for rows (axis = 1) or columns (axis = 0).\n",
    "    \"\"\"\n",
    "    d = matrix.shape[1]\n",
    "\n",
    "    l2_norm = (np.linalg.norm(matrix, axis=axis) * math.sqrt(d) - 1 ) / (math.sqrt(d) - 1)\n",
    "\n",
    "    return l2_norm\n",
    "\n",
    "\n",
    "print (np.round(gof_by_l2_norm(m), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}