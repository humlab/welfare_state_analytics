{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import numpy as np\n","Survival function  (also defined as ``1 - cdf`(Cumulative distribution function.)\n","\n","def chi2(f_obs, f_exp=None, ddof=0, axis=0):\n","    \n","    f_obs = np.asanyarray(f_obs)\n","\n","    if f_exp is not None:\n","        f_exp = np.atleast_1d(np.asanyarray(f_exp))\n","    else:\n","        # assume uniform distribution\n","        f_exp = f_obs.mean(axis=axis, keepdims=True)\n","\n","    terms = (f_obs - f_exp)**2 / f_exp\n","\n","    stat = terms.sum(axis=axis)\n","\n","    num_obs = np.ma.count(terms, axis=axis)\n","    ddof = np.asarray(ddof)\n","    p = distributions.chi2.sf(stat, num_obs - 1 - ddof)\n","\n","    return {'statistic': stat, 'pvalue': p}\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["chi2([1,2,3,4])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Power_divergenceResult = namedtuple('Power_divergenceResult',\n","                                    ('statistic', 'pvalue'))\n","\n","def power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None):\n","    \"\"\"\n","    Cressie-Read power divergence statistic and goodness of fit test.\n","    This function tests the null hypothesis that the categorical data\n","    has the given frequencies, using the Cressie-Read power divergence\n","    statistic.\n","    Parameters\n","    ----------\n","    f_obs : array_like\n","        Observed frequencies in each category.\n","    f_exp : array_like, optional\n","        Expected frequencies in each category.  By default the categories are\n","        assumed to be equally likely.\n","    ddof : int, optional\n","        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n","        for the p-value.  The p-value is computed using a chi-squared\n","        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n","        is the number of observed frequencies.  The default value of `ddof`\n","        is 0.\n","    axis : int or None, optional\n","        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n","        apply the test.  If axis is None, all values in `f_obs` are treated\n","        as a single data set.  Default is 0.\n","    lambda_ : float or str, optional\n","        `lambda_` gives the power in the Cressie-Read power divergence\n","        statistic.  The default is 1.  For convenience, `lambda_` may be\n","        assigned one of the following strings, in which case the\n","        corresponding numerical value is used::\n","            String              Value   Description\n","            \"pearson\"             1     Pearson's chi-squared statistic.\n","                                        In this case, the function is\n","                                        equivalent to `stats.chisquare`.\n","            \"log-likelihood\"      0     Log-likelihood ratio. Also known as\n","                                        the G-test [3]_.\n","            \"freeman-tukey\"      -1/2   Freeman-Tukey statistic.\n","            \"mod-log-likelihood\" -1     Modified log-likelihood ratio.\n","            \"neyman\"             -2     Neyman's statistic.\n","            \"cressie-read\"        2/3   The power recommended in [5]_.\n","    Returns\n","    -------\n","    statistic : float or ndarray\n","        The Cressie-Read power divergence test statistic.  The value is\n","        a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n","    pvalue : float or ndarray\n","        The p-value of the test.  The value is a float if `ddof` and the\n","        return value `stat` are scalars.\n","    See Also\n","    --------\n","    chisquare\n","    Notes\n","    -----\n","    This test is invalid when the observed or expected frequencies in each\n","    category are too small.  A typical rule is that all of the observed\n","    and expected frequencies should be at least 5.\n","    When `lambda_` is less than zero, the formula for the statistic involves\n","    dividing by `f_obs`, so a warning or error may be generated if any value\n","    in `f_obs` is 0.\n","    Similarly, a warning or error may be generated if any value in `f_exp` is\n","    zero when `lambda_` >= 0.\n","    The default degrees of freedom, k-1, are for the case when no parameters\n","    of the distribution are estimated. If p parameters are estimated by\n","    efficient maximum likelihood then the correct degrees of freedom are\n","    k-1-p. If the parameters are estimated in a different way, then the\n","    dof can be between k-1-p and k-1. However, it is also possible that\n","    the asymptotic distribution is not a chisquare, in which case this\n","    test is not appropriate.\n","    This function handles masked arrays.  If an element of `f_obs` or `f_exp`\n","    is masked, then data at that position is ignored, and does not count\n","    towards the size of the data set.\n","    .. versionadded:: 0.13.0\n","    References\n","    ----------\n","    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n","           Statistics\". Chapter 8.\n","           https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n","    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n","    .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n","    .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n","           practice of statistics in biological research\", New York: Freeman\n","           (1981)\n","    .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n","           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n","           pp. 440-464.\n","    Examples\n","    --------\n","    (See `chisquare` for more examples.)\n","    When just `f_obs` is given, it is assumed that the expected frequencies\n","    are uniform and given by the mean of the observed frequencies.  Here we\n","    perform a G-test (i.e. use the log-likelihood ratio statistic):\n","    >>> from scipy.stats import power_divergence\n","    >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n","    (2.006573162632538, 0.84823476779463769)\n","    The expected frequencies can be given with the `f_exp` argument:\n","    >>> power_divergence([16, 18, 16, 14, 12, 12],\n","    ...                  f_exp=[16, 16, 16, 16, 16, 8],\n","    ...                  lambda_='log-likelihood')\n","    (3.3281031458963746, 0.6495419288047497)\n","    When `f_obs` is 2-D, by default the test is applied to each column.\n","    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n","    >>> obs.shape\n","    (6, 2)\n","    >>> power_divergence(obs, lambda_=\"log-likelihood\")\n","    (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n","    By setting ``axis=None``, the test is applied to all data in the array,\n","    which is equivalent to applying the test to the flattened array.\n","    >>> power_divergence(obs, axis=None)\n","    (23.31034482758621, 0.015975692534127565)\n","    >>> power_divergence(obs.ravel())\n","    (23.31034482758621, 0.015975692534127565)\n","    `ddof` is the change to make to the default degrees of freedom.\n","    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n","    (2.0, 0.73575888234288467)\n","    The calculation of the p-values is done by broadcasting the\n","    test statistic with `ddof`.\n","    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n","    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n","    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n","    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n","    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n","    statistics, we must use ``axis=1``:\n","    >>> power_divergence([16, 18, 16, 14, 12, 12],\n","    ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n","    ...                         [8, 20, 20, 16, 12, 12]],\n","    ...                  axis=1)\n","    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n","    \"\"\"\n","    # Convert the input argument `lambda_` to a numerical value.\n","    if isinstance(lambda_, string_types):\n","        if lambda_ not in _power_div_lambda_names:\n","            names = repr(list(_power_div_lambda_names.keys()))[1:-1]\n","            raise ValueError(\"invalid string for lambda_: {0!r}.  Valid strings \"\n","                             \"are {1}\".format(lambda_, names))\n","        lambda_ = _power_div_lambda_names[lambda_]\n","    elif lambda_ is None:\n","        lambda_ = 1\n","\n","    f_obs = np.asanyarray(f_obs)\n","\n","    if f_exp is not None:\n","        f_exp = np.atleast_1d(np.asanyarray(f_exp))\n","    else:\n","        # Compute the equivalent of\n","        #   f_exp = f_obs.mean(axis=axis, keepdims=True)\n","        # Older versions of numpy do not have the 'keepdims' argument, so\n","        # we have to do a little work to achieve the same result.\n","        # Ignore 'invalid' errors so the edge case of a data set with length 0\n","        # is handled without spurious warnings.\n","        with np.errstate(invalid='ignore'):\n","            f_exp = np.atleast_1d(f_obs.mean(axis=axis))\n","        if axis is not None:\n","            reduced_shape = list(f_obs.shape)\n","            reduced_shape[axis] = 1\n","            f_exp.shape = reduced_shape\n","\n","    # `terms` is the array of terms that are summed along `axis` to create\n","    # the test statistic.  We use some specialized code for a few special\n","    # cases of lambda_.\n","    if lambda_ == 1:\n","        # Pearson's chi-squared statistic\n","        terms = (f_obs - f_exp)**2 / f_exp\n","    elif lambda_ == 0:\n","        # Log-likelihood ratio (i.e. G-test)\n","        terms = 2.0 * special.xlogy(f_obs, f_obs / f_exp)\n","    elif lambda_ == -1:\n","        # Modified log-likelihood ratio\n","        terms = 2.0 * special.xlogy(f_exp, f_exp / f_obs)\n","    else:\n","        # General Cressie-Read power divergence.\n","        terms = f_obs * ((f_obs / f_exp)**lambda_ - 1)\n","        terms /= 0.5 * lambda_ * (lambda_ + 1)\n","\n","    stat = terms.sum(axis=axis)\n","\n","    num_obs = _count(terms, axis=axis)\n","    ddof = asarray(ddof)\n","    p = distributions.chi2.sf(stat, num_obs - 1 - ddof)\n","\n","    return Power_divergenceResult(stat, p)\n","\n","\n","def chisquare(f_obs, f_exp=None, ddof=0, axis=0):\n","    \"\"\"\n","    Calculate a one-way chi square test.\n","    The chi square test tests the null hypothesis that the categorical data\n","    has the given frequencies.\n","    Parameters\n","    ----------\n","    f_obs : array_like\n","        Observed frequencies in each category.\n","    f_exp : array_like, optional\n","        Expected frequencies in each category.  By default the categories are\n","        assumed to be equally likely.\n","    ddof : int, optional\n","        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n","        for the p-value.  The p-value is computed using a chi-squared\n","        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n","        is the number of observed frequencies.  The default value of `ddof`\n","        is 0.\n","    axis : int or None, optional\n","        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n","        apply the test.  If axis is None, all values in `f_obs` are treated\n","        as a single data set.  Default is 0.\n","    Returns\n","    -------\n","    chisq : float or ndarray\n","        The chi-squared test statistic.  The value is a float if `axis` is\n","        None or `f_obs` and `f_exp` are 1-D.\n","    p : float or ndarray\n","        The p-value of the test.  The value is a float if `ddof` and the\n","        return value `chisq` are scalars.\n","    See Also\n","    --------\n","    scipy.stats.power_divergence\n","    Notes\n","    -----\n","    This test is invalid when the observed or expected frequencies in each\n","    category are too small.  A typical rule is that all of the observed\n","    and expected frequencies should be at least 5.\n","    The default degrees of freedom, k-1, are for the case when no parameters\n","    of the distribution are estimated. If p parameters are estimated by\n","    efficient maximum likelihood then the correct degrees of freedom are\n","    k-1-p. If the parameters are estimated in a different way, then the\n","    dof can be between k-1-p and k-1. However, it is also possible that\n","    the asymptotic distribution is not a chisquare, in which case this\n","    test is not appropriate.\n","    References\n","    ----------\n","    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n","           Statistics\". Chapter 8.\n","           https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html\n","    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n","    Examples\n","    --------\n","    When just `f_obs` is given, it is assumed that the expected frequencies\n","    are uniform and given by the mean of the observed frequencies.\n","    >>> from scipy.stats import chisquare\n","    >>> chisquare([16, 18, 16, 14, 12, 12])\n","    (2.0, 0.84914503608460956)\n","    With `f_exp` the expected frequencies can be given.\n","    >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n","    (3.5, 0.62338762774958223)\n","    When `f_obs` is 2-D, by default the test is applied to each column.\n","    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n","    >>> obs.shape\n","    (6, 2)\n","    >>> chisquare(obs)\n","    (array([ 2.        ,  6.66666667]), array([ 0.84914504,  0.24663415]))\n","    By setting ``axis=None``, the test is applied to all data in the array,\n","    which is equivalent to applying the test to the flattened array.\n","    >>> chisquare(obs, axis=None)\n","    (23.31034482758621, 0.015975692534127565)\n","    >>> chisquare(obs.ravel())\n","    (23.31034482758621, 0.015975692534127565)\n","    `ddof` is the change to make to the default degrees of freedom.\n","    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n","    (2.0, 0.73575888234288467)\n","    The calculation of the p-values is done by broadcasting the\n","    chi-squared statistic with `ddof`.\n","    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n","    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n","    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n","    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n","    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n","    statistics, we use ``axis=1``:\n","    >>> chisquare([16, 18, 16, 14, 12, 12],\n","    ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n","    ...           axis=1)\n","    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n","    \"\"\"\n","    return power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis,\n","                            lambda_=\"pearson\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}