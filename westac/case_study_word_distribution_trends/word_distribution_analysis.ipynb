{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Word frequency distribution trends"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"%load_ext autoreload\n%autoreload 2\n\nimport os, sys\n\nimport westac.common.corpus_vectorizer as corpus_vectorizer\nimport westac.common.text_corpus as text_corpus\nimport westac.common.utility as cutility\nimport numpy as np\nimport sklearn\n\n"},{"cell_type":"markdown","metadata":{},"source":"# Helpers"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"def create_corpus(filename):\n    meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n    reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n    kwargs = dict(isalnum=False, to_lower=False, deacc=False, min_len=2, max_len=None, numerals=False)\n    corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n    return corpus"},{"cell_type":"markdown","metadata":{},"source":["# Analysis\n","https://github.com/davidmcclure/lint-analysis/tree/master/notebooks/2017\n"]},{"cell_type":"markdown","metadata":{},"source":["## Goodness-of-fit to uniform distribution (chi-square)\n","\n","See [scipy.stats.chisquare](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html): \n","\"*When just f_obs is given, it is assumed that the expected frequencies are uniform...*\"\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"import numpy as np\nfrom scipy import stats\n\n#filename = './test/test_data/test_corpus.zip'\n#filename = './data/Sample_1945-1989_1.zip'\n\nfilename = './data/SOU_1945-1989.zip'\ndump_name = os.path.basename(filename).split('.')[0]\n\nvectorizer = corpus_vectorizer.CorpusVectorizer()\ncorpus = create_corpus(filename)\nX = vectorizer.fit_transform(corpus)\nvectorizer.dump(dump_name, folder='./output')\n\n#vectorizer.load(dump_name, folder='./output')\n\n\nif False:\n    \n    Y         = vectorizer.collapse_to_year()\n    Yn        = vectorizer.normalize(Y, axis=1, norm='l1') \n    Ynw       = vectorizer.slice_tokens_by_count_threshold(Yn, 1)\n    Yx2, imap = vectorizer.pick_by_top_variance(500)\n    \n    stats.chisquare(Ynw, f_exp=None, ddof=0, axis=0)\n"},{"cell_type":"markdown","metadata":{},"source":"# Clustering (Ward and K-means)\n\nSee [this](https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/) tutorial.\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"from scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\n\nlinked = linkage(Z.todense(), 'ward')\n\nlabelList = tokens_of_interest\n\nplt.figure(figsize=(10, 7))\ndendrogram(linked, orientation='top', labels=labelList, distance_sort='descending', show_leaf_counts=True)\nplt.show()\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# df_Zy.sum().where(lambda x: x>= 10000).sort_values().dropna()"},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":2},"outputs":[],"source":"#Xn = normalize(X, axis=1, norm='l1')\n#Y = collapse_to_year_matrix(X, df_documents)\n#df = pd.DataFrame(Y, columns=list(vectorizer.get_feature_names()))\n#df.to_excel('test.xlsx')\n\nif False:\n    \n    df = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names()))\n    df['year'] = df.index + 45\n    df = df.set_index('year')\n    df['year'] =  pd.Series(df.index).apply(lambda x: documents[x][0])\n    %matplotlib inline\n    df[['krig']].plot() #.loc[df[\"000\"]==49]\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}