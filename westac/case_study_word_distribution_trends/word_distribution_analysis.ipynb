{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Word frequency distribution trends"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"%load_ext autoreload\n%autoreload 2\nimport os, sys\nimport westac.common.corpus_vectorizer as corpus_vectorizer\nimport westac.common.text_corpus as text_corpus\nimport westac.common.utility as utility\nimport numpy as np\nimport sklearn"},{"cell_type":"markdown","metadata":{},"source":"# Analysis\nhttps://github.com/davidmcclure/lint-analysis/tree/master/notebooks/2017\n"},{"cell_type":"markdown","metadata":{},"source":["## Goodness-of-fit to uniform distribution (chi-square)\n","\n","See [scipy.stats.chisquare](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html): \n","\"*When just f_obs is given, it is assumed that the expected frequencies are uniform...*\"\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Loading data matrix...\n"}],"source":"import os\nimport westac.common.corpus_vectorizer as corpus_vectorizer\nimport westac.common.text_corpus as text_corpus\n\nfrom scipy import stats\n\ndef vectorize_corpus(filename):\n\n    if not os.path.isfile(filename):\n        print('error: no such file: {}'.format(filename))\n        assert os.path.isfile(filename)\n\n    dump_tag = os.path.basename(filename).split('.')[0]\n\n    vectorizer = corpus_vectorizer.CorpusVectorizer()\n\n    if not vectorizer.dump_exists(dump_tag):\n\n        meta_extract = {\n            'year': r\"SOU (\\d{4})\\_.*\",\n            'serial_no': r\"SOU \\d{4}\\_(\\d+).*\"\n        }\n\n        print('Creating new corpus...')\n        corpus = text_corpus.create_corpus(filename, meta_extract)\n\n        print('Creating document-term matrix...')\n        _ = vectorizer.fit_transform(corpus)\n\n        print('Saving data matrix...')\n        vectorizer.dump(tag=dump_tag, folder='./output')\n\n    else:\n\n        print('Loading data matrix...')\n\n        vectorizer.load(dump_tag, folder='./output')\n\n    print('Done!')\n    return vectorizer \n\nfilename = './data/SOU_1945-1989.zip'\nvectorizer = vectorize_corpus(filename)\n"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":"True"},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":""},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"\nyear_term_matrix   = vectorizer.collapse_to_year()\nyear_term_matrix_n = vectorizer.normalize(year_term_matrix, axis=1, norm='l1')\n# Y, categories = vectorizer.collapse_by_category('year')\n# (Y == year_term_matrix).all()\n\n#Ynw       = vectorizer.slice_tokens_by_count_threshold(Yn, 1)\n\n#Yx2, imap = vectorizer.pick_by_top_variance(500)\n\n#data       = stats.chisquare(Ynw, f_exp=None, ddof=0, axis=0)\n"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"copy_x : boolean, optional\nWhen pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown.\n\nalgorithm : “auto”, “full” or “elkan”, default=”auto”\nK-means algorithm to use. The classical EM-style algorithm is “full”. The “elkan” variation is more efficient by using the triangle inequality, but currently doesn’t support sparse data. “auto” chooses “elkan” for dense data and “full” for sparse data.\n"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"(45, 5248040)\n[1 1 1 1 1 1 1 1 3 3 3 3 3 4 3 3 3 3 4 4 4 4 4 0 0 0 0 0 0 2 2 2 2 2 2 2 2\n 5 5 5 5 5 5 5 5]\n0.0007008333416259124\n6\n"}],"source":"from sklearn.cluster import KMeans\nimport numpy as np\n\n# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n\nprint(year_term_matrix_n.shape)\nkmeans = KMeans(n_clusters=6, random_state=1337, precompute_distances='auto', n_init=10, n_jobs=4, algorithm='auto')\\\n    .fit(year_term_matrix_n)\n\nprint(kmeans.labels_)\nprint(kmeans.inertia_)\nprint(len(kmeans.cluster_centers_))\n\n# kmeans.predict()"},{"cell_type":"markdown","metadata":{},"source":"# Clustering (Ward and K-means)\n\nSee [this](https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/) tutorial.\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"from scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\n\nlinked = linkage(Z.todense(), 'ward')\n\nlabelList = tokens_of_interest\n\nplt.figure(figsize=(10, 7))\ndendrogram(linked, orientation='top', labels=labelList, distance_sort='descending', show_leaf_counts=True)\nplt.show()\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# df_Zy.sum().where(lambda x: x>= 10000).sort_values().dropna()"},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":2},"outputs":[],"source":"#Xn = normalize(X, axis=1, norm='l1')\n#Y = collapse_to_year_matrix(X, df_documents)\n#df = pd.DataFrame(Y, columns=list(vectorizer.get_feature_names()))\n#df.to_excel('test.xlsx')\n\nif False:\n    \n    df = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names()))\n    df['year'] = df.index + 45\n    df = df.set_index('year')\n    df['year'] =  pd.Series(df.index).apply(lambda x: documents[x][0])\n    %matplotlib inline\n    df[['krig']].plot() #.loc[df[\"000\"]==49]\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}