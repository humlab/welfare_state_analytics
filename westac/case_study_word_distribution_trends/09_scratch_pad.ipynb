{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import numpy as np\n","Survival function  (also defined as ``1 - cdf`(Cumulative distribution function.)\n","\n","def chi2(f_obs, f_exp=None, ddof=0, axis=0):\n","    \n","    f_obs = np.asanyarray(f_obs)\n","\n","    if f_exp is not None:\n","        f_exp = np.atleast_1d(np.asanyarray(f_exp))\n","    else:\n","        # assume uniform distribution\n","        f_exp = f_obs.mean(axis=axis, keepdims=True)\n","\n","    terms = (f_obs - f_exp)**2 / f_exp\n","\n","    stat = terms.sum(axis=axis)\n","\n","    num_obs = np.ma.count(terms, axis=axis)\n","    ddof = np.asarray(ddof)\n","    p = distributions.chi2.sf(stat, num_obs - 1 - ddof)\n","\n","    return {'statistic': stat, 'pvalue': p}\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Create 50 datapoints in two clusters a and b\n","import numpy as np\n","from scipy.cluster.vq import vq, kmeans, whiten\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","features  = array([[ 1.9, 2.3],\n","                   [ 1.5, 2.5],\n","                   [ 0.8, 0.6],\n","                   [ 0.4, 1.8],\n","                   [ 0.1, 0.1],\n","                   [ 0.2, 1.8],\n","                   [ 2.0, 0.5],\n","                   [ 0.3, 1.5],\n","                   [ 1.0, 1.0]])\n","\n","whitened = whiten(features)\n","book = np.array((whitened[0], whitened[2]))\n","\n","kmeans(whitened,book)\n","\n","pts = 50\n","a = np.random.multivariate_normal([0, 0], [[4, 1], [1, 4]], size=pts)\n","b = np.random.multivariate_normal([30, 10],[[10, 2], [2, 1]], size=pts)\n","features = np.concatenate((a, b))\n","\n","# Whiten data\n","whitened = whiten(features)\n","\n","# Find 2 clusters in the data\n","codebook, distortion = kmeans(whitened, 2)\n","\n","# Plot whitened data and cluster centers in red\n","plt.scatter(whitened[:, 0], whitened[:, 1])\n","plt.scatter(codebook[:, 0], codebook[:, 1], c='r')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import itertools\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def main():\n","    np.random.seed(1977)\n","    numvars, numdata = 4, 10\n","    data = 10 * np.random.random((numvars, numdata))\n","    fig = scatterplot_matrix(data, ['mpg', 'disp', 'drat', 'wt'],\n","            linestyle='none', marker='o', color='black', mfc='none')\n","    fig.suptitle('Simple Scatterplot Matrix')\n","    plt.show()\n","\n","def scatterplot_matrix(data, names, **kwargs):\n","    \"\"\"Plots a scatterplot matrix of subplots.  Each row of \"data\" is plotted\n","    against other rows, resulting in a nrows by nrows grid of subplots with the\n","    diagonal subplots labeled with \"names\".  Additional keyword arguments are\n","    passed on to matplotlib's \"plot\" command. Returns the matplotlib figure\n","    object containg the subplot grid.\"\"\"\n","    numvars, numdata = data.shape\n","    fig, axes = plt.subplots(nrows=numvars, ncols=numvars, figsize=(8,8))\n","    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n","\n","    for ax in axes.flat:\n","        # Hide all ticks and labels\n","        ax.xaxis.set_visible(False)\n","        ax.yaxis.set_visible(False)\n","\n","        # Set up ticks only on one side for the \"edge\" subplots...\n","        if ax.is_first_col():\n","            ax.yaxis.set_ticks_position('left')\n","        if ax.is_last_col():\n","            ax.yaxis.set_ticks_position('right')\n","        if ax.is_first_row():\n","            ax.xaxis.set_ticks_position('top')\n","        if ax.is_last_row():\n","            ax.xaxis.set_ticks_position('bottom')\n","\n","    # Plot the data.\n","    for i, j in zip(*np.triu_indices_from(axes, k=1)):\n","        for x, y in [(i,j), (j,i)]:\n","            axes[x,y].plot(data[x], data[y], **kwargs)\n","\n","    # Label the diagonal subplots...\n","    for i, label in enumerate(names):\n","        axes[i,i].annotate(label, (0.5, 0.5), xycoords='axes fraction',\n","                ha='center', va='center')\n","\n","    # Turn on the proper x or y axes ticks.\n","    for i, j in zip(range(numvars), itertools.cycle((-1, 0))):\n","        axes[j,i].xaxis.set_visible(True)\n","        axes[i,j].yaxis.set_visible(True)\n","\n","    return fig\n","\n","main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["chi2([1,2,3,4])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Power_divergenceResult = namedtuple('Power_divergenceResult',\n","                                    ('statistic', 'pvalue'))\n","\n","def power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None):\n","    \"\"\"\n","    Cressie-Read power divergence statistic and goodness of fit test.\n","    This function tests the null hypothesis that the categorical data\n","    has the given frequencies, using the Cressie-Read power divergence\n","    statistic.\n","    Parameters\n","    ----------\n","    f_obs : array_like\n","        Observed frequencies in each category.\n","    f_exp : array_like, optional\n","        Expected frequencies in each category.  By default the categories are\n","        assumed to be equally likely.\n","    ddof : int, optional\n","        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n","        for the p-value.  The p-value is computed using a chi-squared\n","        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n","        is the number of observed frequencies.  The default value of `ddof`\n","        is 0.\n","    axis : int or None, optional\n","        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n","        apply the test.  If axis is None, all values in `f_obs` are treated\n","        as a single data set.  Default is 0.\n","    lambda_ : float or str, optional\n","        `lambda_` gives the power in the Cressie-Read power divergence\n","        statistic.  The default is 1.  For convenience, `lambda_` may be\n","        assigned one of the following strings, in which case the\n","        corresponding numerical value is used::\n","            String              Value   Description\n","            \"pearson\"             1     Pearson's chi-squared statistic.\n","                                        In this case, the function is\n","                                        equivalent to `stats.chisquare`.\n","            \"log-likelihood\"      0     Log-likelihood ratio. Also known as\n","                                        the G-test [3]_.\n","            \"freeman-tukey\"      -1/2   Freeman-Tukey statistic.\n","            \"mod-log-likelihood\" -1     Modified log-likelihood ratio.\n","            \"neyman\"             -2     Neyman's statistic.\n","            \"cressie-read\"        2/3   The power recommended in [5]_.\n","    Returns\n","    -------\n","    statistic : float or ndarray\n","        The Cressie-Read power divergence test statistic.  The value is\n","        a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n","    pvalue : float or ndarray\n","        The p-value of the test.  The value is a float if `ddof` and the\n","        return value `stat` are scalars.\n","    See Also\n","    --------\n","    chisquare\n","    Notes\n","    -----\n","    This test is invalid when the observed or expected frequencies in each\n","    category are too small.  A typical rule is that all of the observed\n","    and expected frequencies should be at least 5.\n","    When `lambda_` is less than zero, the formula for the statistic involves\n","    dividing by `f_obs`, so a warning or error may be generated if any value\n","    in `f_obs` is 0.\n","    Similarly, a warning or error may be generated if any value in `f_exp` is\n","    zero when `lambda_` >= 0.\n","    The default degrees of freedom, k-1, are for the case when no parameters\n","    of the distribution are estimated. If p parameters are estimated by\n","    efficient maximum likelihood then the correct degrees of freedom are\n","    k-1-p. If the parameters are estimated in a different way, then the\n","    dof can be between k-1-p and k-1. However, it is also possible that\n","    the asymptotic distribution is not a chisquare, in which case this\n","    test is not appropriate.\n","    This function handles masked arrays.  If an element of `f_obs` or `f_exp`\n","    is masked, then data at that position is ignored, and does not count\n","    towards the size of the data set.\n","    .. versionadded:: 0.13.0\n","    References\n","    ----------\n","    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n","           Statistics\". Chapter 8.\n","           https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n","    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n","    .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n","    .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n","           practice of statistics in biological research\", New York: Freeman\n","           (1981)\n","    .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n","           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n","           pp. 440-464.\n","    Examples\n","    --------\n","    (See `chisquare` for more examples.)\n","    When just `f_obs` is given, it is assumed that the expected frequencies\n","    are uniform and given by the mean of the observed frequencies.  Here we\n","    perform a G-test (i.e. use the log-likelihood ratio statistic):\n","    >>> from scipy.stats import power_divergence\n","    >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n","    (2.006573162632538, 0.84823476779463769)\n","    The expected frequencies can be given with the `f_exp` argument:\n","    >>> power_divergence([16, 18, 16, 14, 12, 12],\n","    ...                  f_exp=[16, 16, 16, 16, 16, 8],\n","    ...                  lambda_='log-likelihood')\n","    (3.3281031458963746, 0.6495419288047497)\n","    When `f_obs` is 2-D, by default the test is applied to each column.\n","    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n","    >>> obs.shape\n","    (6, 2)\n","    >>> power_divergence(obs, lambda_=\"log-likelihood\")\n","    (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n","    By setting ``axis=None``, the test is applied to all data in the array,\n","    which is equivalent to applying the test to the flattened array.\n","    >>> power_divergence(obs, axis=None)\n","    (23.31034482758621, 0.015975692534127565)\n","    >>> power_divergence(obs.ravel())\n","    (23.31034482758621, 0.015975692534127565)\n","    `ddof` is the change to make to the default degrees of freedom.\n","    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n","    (2.0, 0.73575888234288467)\n","    The calculation of the p-values is done by broadcasting the\n","    test statistic with `ddof`.\n","    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n","    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n","    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n","    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n","    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n","    statistics, we must use ``axis=1``:\n","    >>> power_divergence([16, 18, 16, 14, 12, 12],\n","    ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n","    ...                         [8, 20, 20, 16, 12, 12]],\n","    ...                  axis=1)\n","    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n","    \"\"\"\n","    # Convert the input argument `lambda_` to a numerical value.\n","    if isinstance(lambda_, string_types):\n","        if lambda_ not in _power_div_lambda_names:\n","            names = repr(list(_power_div_lambda_names.keys()))[1:-1]\n","            raise ValueError(\"invalid string for lambda_: {0!r}.  Valid strings \"\n","                             \"are {1}\".format(lambda_, names))\n","        lambda_ = _power_div_lambda_names[lambda_]\n","    elif lambda_ is None:\n","        lambda_ = 1\n","\n","    f_obs = np.asanyarray(f_obs)\n","\n","    if f_exp is not None:\n","        f_exp = np.atleast_1d(np.asanyarray(f_exp))\n","    else:\n","        # Compute the equivalent of\n","        #   f_exp = f_obs.mean(axis=axis, keepdims=True)\n","        # Older versions of numpy do not have the 'keepdims' argument, so\n","        # we have to do a little work to achieve the same result.\n","        # Ignore 'invalid' errors so the edge case of a data set with length 0\n","        # is handled without spurious warnings.\n","        with np.errstate(invalid='ignore'):\n","            f_exp = np.atleast_1d(f_obs.mean(axis=axis))\n","        if axis is not None:\n","            reduced_shape = list(f_obs.shape)\n","            reduced_shape[axis] = 1\n","            f_exp.shape = reduced_shape\n","\n","    # `terms` is the array of terms that are summed along `axis` to create\n","    # the test statistic.  We use some specialized code for a few special\n","    # cases of lambda_.\n","    if lambda_ == 1:\n","        # Pearson's chi-squared statistic\n","        terms = (f_obs - f_exp)**2 / f_exp\n","    elif lambda_ == 0:\n","        # Log-likelihood ratio (i.e. G-test)\n","        terms = 2.0 * special.xlogy(f_obs, f_obs / f_exp)\n","    elif lambda_ == -1:\n","        # Modified log-likelihood ratio\n","        terms = 2.0 * special.xlogy(f_exp, f_exp / f_obs)\n","    else:\n","        # General Cressie-Read power divergence.\n","        terms = f_obs * ((f_obs / f_exp)**lambda_ - 1)\n","        terms /= 0.5 * lambda_ * (lambda_ + 1)\n","\n","    stat = terms.sum(axis=axis)\n","\n","    num_obs = _count(terms, axis=axis)\n","    ddof = asarray(ddof)\n","    p = distributions.chi2.sf(stat, num_obs - 1 - ddof)\n","\n","    return Power_divergenceResult(stat, p)\n","\n","\n","def chisquare(f_obs, f_exp=None, ddof=0, axis=0):\n","    \"\"\"\n","    Calculate a one-way chi square test.\n","    The chi square test tests the null hypothesis that the categorical data\n","    has the given frequencies.\n","    Parameters\n","    ----------\n","    f_obs : array_like\n","        Observed frequencies in each category.\n","    f_exp : array_like, optional\n","        Expected frequencies in each category.  By default the categories are\n","        assumed to be equally likely.\n","    ddof : int, optional\n","        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n","        for the p-value.  The p-value is computed using a chi-squared\n","        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n","        is the number of observed frequencies.  The default value of `ddof`\n","        is 0.\n","    axis : int or None, optional\n","        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n","        apply the test.  If axis is None, all values in `f_obs` are treated\n","        as a single data set.  Default is 0.\n","    Returns\n","    -------\n","    chisq : float or ndarray\n","        The chi-squared test statistic.  The value is a float if `axis` is\n","        None or `f_obs` and `f_exp` are 1-D.\n","    p : float or ndarray\n","        The p-value of the test.  The value is a float if `ddof` and the\n","        return value `chisq` are scalars.\n","    See Also\n","    --------\n","    scipy.stats.power_divergence\n","    Notes\n","    -----\n","    This test is invalid when the observed or expected frequencies in each\n","    category are too small.  A typical rule is that all of the observed\n","    and expected frequencies should be at least 5.\n","    The default degrees of freedom, k-1, are for the case when no parameters\n","    of the distribution are estimated. If p parameters are estimated by\n","    efficient maximum likelihood then the correct degrees of freedom are\n","    k-1-p. If the parameters are estimated in a different way, then the\n","    dof can be between k-1-p and k-1. However, it is also possible that\n","    the asymptotic distribution is not a chisquare, in which case this\n","    test is not appropriate.\n","    References\n","    ----------\n","    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n","           Statistics\". Chapter 8.\n","           https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html\n","    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n","    Examples\n","    --------\n","    When just `f_obs` is given, it is assumed that the expected frequencies\n","    are uniform and given by the mean of the observed frequencies.\n","    >>> from scipy.stats import chisquare\n","    >>> chisquare([16, 18, 16, 14, 12, 12])\n","    (2.0, 0.84914503608460956)\n","    With `f_exp` the expected frequencies can be given.\n","    >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n","    (3.5, 0.62338762774958223)\n","    When `f_obs` is 2-D, by default the test is applied to each column.\n","    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n","    >>> obs.shape\n","    (6, 2)\n","    >>> chisquare(obs)\n","    (array([ 2.        ,  6.66666667]), array([ 0.84914504,  0.24663415]))\n","    By setting ``axis=None``, the test is applied to all data in the array,\n","    which is equivalent to applying the test to the flattened array.\n","    >>> chisquare(obs, axis=None)\n","    (23.31034482758621, 0.015975692534127565)\n","    >>> chisquare(obs.ravel())\n","    (23.31034482758621, 0.015975692534127565)\n","    `ddof` is the change to make to the default degrees of freedom.\n","    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n","    (2.0, 0.73575888234288467)\n","    The calculation of the p-values is done by broadcasting the\n","    chi-squared statistic with `ddof`.\n","    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n","    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n","    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n","    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n","    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n","    statistics, we use ``axis=1``:\n","    >>> chisquare([16, 18, 16, 14, 12, 12],\n","    ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n","    ...           axis=1)\n","    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n","    \"\"\"\n","    return power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis,\n","                            lambda_=\"pearson\")\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":"\n\n\n\n\n\n  <div class=\"bk-root\" id=\"b79fec08-d5d4-4388-8911-9b5425c76e13\" data-root-id=\"1758\"></div>\n"},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":"(function(root) {\n  function embed_document(root) {\n    \n  var docs_json = {\"8fde255a-6058-487a-8ba4-2fb903eb7ddc\":{\"roots\":{\"references\":[{\"attributes\":{\"above\":[{\"id\":\"1784\",\"type\":\"Title\"}],\"background_fill_color\":{\"value\":\"white\"},\"below\":[{\"id\":\"1767\",\"type\":\"LinearAxis\"},{\"id\":\"1783\",\"type\":\"Label\"}],\"border_fill_color\":{\"value\":\"white\"},\"center\":[{\"id\":\"1771\",\"type\":\"Grid\"},{\"id\":\"1776\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"1772\",\"type\":\"LinearAxis\"}],\"min_border_bottom\":60,\"min_border_left\":60,\"min_border_right\":60,\"min_border_top\":40,\"outline_line_color\":{\"value\":\"white\"},\"plot_height\":540,\"plot_width\":960,\"renderers\":[{\"id\":\"1792\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1780\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1778\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1759\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1763\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1761\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1765\",\"type\":\"LinearScale\"}},\"id\":\"1758\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"text\":\"Takeaway\",\"text_color\":{\"value\":\"#333333\"},\"text_font_size\":{\"value\":\"18pt\"}},\"id\":\"1780\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1842\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data_source\":{\"id\":\"1788\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1790\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1791\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"1793\",\"type\":\"CDSView\"}},\"id\":\"1792\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":null,\"ticker\":{\"id\":\"1773\",\"type\":\"BasicTicker\"}},\"id\":\"1776\",\"type\":\"Grid\"},{\"attributes\":{\"text\":\"This is my way\",\"text_color\":{\"value\":\"#666666\"},\"text_font_size\":{\"value\":\"12pt\"}},\"id\":\"1784\",\"type\":\"Title\"},{\"attributes\":{\"text_alpha\":{\"value\":0.1},\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"1em\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1791\",\"type\":\"Text\"},{\"attributes\":{\"callback\":null,\"data\":{\"text\":[\"hi\"],\"x\":[10],\"y\":[10]},\"selected\":{\"id\":\"1845\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1844\",\"type\":\"UnionRenderers\"}},\"id\":\"1788\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"axis_label\":\"My why data\",\"axis_label_text_color\":{\"value\":\"#666666\"},\"axis_label_text_font_size\":{\"value\":\"11pt\"},\"axis_label_text_font_style\":\"bold\",\"axis_line_color\":{\"value\":\"#C0C0C0\"},\"formatter\":{\"id\":\"1840\",\"type\":\"BasicTickFormatter\"},\"major_label_text_color\":{\"value\":\"#898989\"},\"major_label_text_font_size\":{\"value\":\"10pt\"},\"major_tick_in\":0,\"major_tick_line_color\":{\"value\":\"#C0C0C0\"},\"major_tick_out\":4,\"minor_tick_line_color\":{\"value\":\"#C0C0C0\"},\"minor_tick_out\":1,\"ticker\":{\"id\":\"1773\",\"type\":\"BasicTicker\"}},\"id\":\"1772\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1768\",\"type\":\"BasicTicker\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"1768\",\"type\":\"BasicTicker\"}},\"id\":\"1771\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1777\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1845\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1765\",\"type\":\"LinearScale\"},{\"attributes\":{\"level\":\"overlay\",\"name\":\"subtitle\",\"text\":\"This is my data\",\"text_align\":\"right\",\"text_color\":{\"value\":\"#898989\"},\"text_font_size\":{\"value\":\"10px\"},\"x\":864.0,\"x_units\":\"screen\",\"y\":0,\"y_units\":\"screen\"},\"id\":\"1783\",\"type\":\"Label\"},{\"attributes\":{},\"id\":\"1840\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"text_color\":{\"value\":\"#000000\"},\"text_font_size\":{\"value\":\"1em\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1790\",\"type\":\"Text\"},{\"attributes\":{\"callback\":null},\"id\":\"1761\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null},\"id\":\"1759\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1844\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"source\":{\"id\":\"1788\",\"type\":\"ColumnDataSource\"}},\"id\":\"1793\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1763\",\"type\":\"LinearScale\"},{\"attributes\":{\"active_drag\":null,\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"logo\":null,\"tools\":[{\"id\":\"1777\",\"type\":\"SaveTool\"}]},\"id\":\"1778\",\"type\":\"Toolbar\"},{\"attributes\":{\"axis_label\":\"My ex data\",\"axis_label_text_color\":{\"value\":\"#666666\"},\"axis_label_text_font_size\":{\"value\":\"11pt\"},\"axis_label_text_font_style\":\"bold\",\"axis_line_color\":{\"value\":\"#C0C0C0\"},\"formatter\":{\"id\":\"1842\",\"type\":\"BasicTickFormatter\"},\"major_label_text_color\":{\"value\":\"#898989\"},\"major_label_text_font_size\":{\"value\":\"10pt\"},\"major_tick_in\":0,\"major_tick_line_color\":{\"value\":\"#C0C0C0\"},\"major_tick_out\":4,\"minor_tick_line_color\":{\"value\":\"#C0C0C0\"},\"minor_tick_out\":1,\"ticker\":{\"id\":\"1768\",\"type\":\"BasicTicker\"}},\"id\":\"1767\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1773\",\"type\":\"BasicTicker\"}],\"root_ids\":[\"1758\"]},\"title\":\"Bokeh Application\",\"version\":\"1.4.0\"}};\n  var render_items = [{\"docid\":\"8fde255a-6058-487a-8ba4-2fb903eb7ddc\",\"roots\":{\"1758\":\"b79fec08-d5d4-4388-8911-9b5425c76e13\"}}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    var attempts = 0;\n    var timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);","application/vnd.bokehjs_exec.v0+json":""},"metadata":{"application/vnd.bokehjs_exec.v0+json":{"id":"1758"}},"output_type":"display_data"}],"source":["import chartify\n","ch = (\n","     chartify.Chart()\n","    .set_title(\"Takeaway\")\n","    .set_subtitle(\"This is my way\")\n","    .axes.set_yaxis_label(\"My why data\")\n","    .axes.set_xaxis_label(\"My ex data\")\n","    .set_source_label(\"This is my data\")\n","    .callout.text('hi', 10, 10)\n","    .show()\n",")\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"box\nline\nline_segment\ntext\n"}],"source":["print('\\n'.join([x for x in dir(ch.call) if not x.startswith('_')]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}