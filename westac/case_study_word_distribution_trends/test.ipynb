{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"\n# Issue: [Word Distribution Trends](https://github.com/humlab/welfare_state_analytics/issues/10)\n\nTo consider:\n\n- How to normalize the distributions? Normalize each words distribution to 1.\n- What clustering to use? Ward? Try K-means to start with.\n- UX two select different kinds of clusters\n\n## Find words that deviates the most from uniform distribution\n1. Compute using chi-square test\n1. Visualisera distribution of \"godness value\" (\"how many word deviates how much\")\n1. List words that for which the null-hypothesis is true 0.05\n\n## Compute distribution clusters for most deviating words\n1. Select n (=500) words that deviates the most.\n1. Select n clusters that covers 9+% all words.\n1. Visualize single cluster\n    - Mean / scatter / box-plot\n1. Visualize all clusters\n    - Each represented as a mean curve\n1. Export individual words for each cluster\n\nNotes:\n\n- Word filters\n\nReferences\n----------\n\n- [1] Lowry, Richard.  \"Concepts and Applications of Inferential Statistics\". Chapter 8. https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html\n- [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n\n"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"(45, 10000)\n"}],"source":"import operator\n\nfrom westac.common import vectorized_corpus\n\nimport scipy\nimport numpy as np\nimport pandas as pd\n\nv_corpus = vectorized_corpus.VectorizedCorpus\\\n            .load('SOU_1945-1989_A', folder='./output')\\\n            .group_by_year()\\\n            .slice_by_n_top(10000)\n            .normalize(axis=1)\n\n            #.slice_by_n_count(1000)\\\n            \nprint(v_corpus.bag_term_matrix.shape)\n\n"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"some keyword arguments unexpected","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-51773b1dd365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#' '.join(v_corpus.n_top_tokens(100).keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# v_corpus.token2id['av']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mv_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_by_n_top\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbag_term_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: some keyword arguments unexpected"]}],"source":"#' '.join(v_corpus.n_top_tokens(100).keys())\n# v_corpus.token2id['av']\nv_corpus.slice_by_n_top(100).normalize(axis=1).bag_term_matrix"},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":"chisq, p_value = scipy.stats.chisquare(v_corpus.term_bag_matrix, f_exp=None, ddof=0, axis=1) # pylint: disable=unused-variable"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"print(v_corpus.bag_term_matrix.shape)\n#print('Counts: {}'.format(len(chisq)))\n#print(chisq[:20])\n#print(p_value[:20])\nprint([ v_corpus.id2token[i] for i in range(0,21) ])\nprint([ v_corpus.word_counts[v_corpus.id2token[i]] for i in range(0,21) ])\n"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"def stat_count_by_n_count(v_corpus, n_bins=100, cumulative=False):\n\n    # n_bins = max(v_corpus.word_counts.iteritems(), key=operator.itemgetter(1))[0]\n\n    bins = [ 0 ] * (n_bins + 1)\n\n    for _, n_count in v_corpus.word_counts.items():\n        bins[min(n_count, n_bins - 1)] += 1\n\n    if cumulative:\n        bins = np.cumsum(bins).tolist()\n\n    return bins\n\ndef letters_in_corpus(v_corpus):\n    letters = {}\n    for w,n in v_corpus.word_counts.items():\n        for c in w:\n            letters[c] = letters.get(c,0) + n\n    return ''.join(list(sorted(letters.keys())))\n\nstat_count_by_n_count(v_corpus)\n"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"\npd.DataFrame({ 'n_count': n_counts }).plot()\n"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import unittest\nimport numpy as np\nimport pandas as pd\n\n\nimport scipy\nfrom scipy.cluster.hierarchy import dendrogram, linkage # pylint: disable=unused-import\nfrom matplotlib import pyplot as plt # pylint: disable=unused-import\n\n        v_corpus = vectorizer\\\n            .fit_transform(corpus)\\\n            .group_by_year()\\\n            .normalize()\\\n            .slice_by_n_count(0)\n\n        X2 = scipy.stats.chisquare(v_corpus.term_bag_matrix, f_exp=None, ddof=0, axis=0) # pylint: disable=unused-variable\n\n        # Use X2 so select top 500 words... (highest Power-Power_divergenceResult)\n        # Ynw = largest_by_chisquare()\n        #print(Ynw)\n\n        linked = linkage(v_corpus.term_bag_matrix, 'ward') # pylint: disable=unused-variable\n        #print(linked)\n\n        ##labels = [ v_corpus.id2token[x] for x in indices ] # pylint: disable=unused-variable\n\n        #plt.figure(figsize=(24, 16))\n        #dendrogram(linked, orientation='top', labels=labels, distance_sort='descending', show_leaf_counts=True)\n        #plt.show()\n\ndef plot_dists(v_corpus):\n    df = pd.DataFrame(v_corpus.bag_term_matrix.toarray(), columns=list(v_corpus.get_feature_names()))\n    df['year'] = df.index + 45\n    df = df.set_index('year')\n    df['year'] =  pd.Series(df.index).apply(lambda x: v_corpus.document_index[x][0])\n    df[['krig']].plot() #.loc[df[\"000\"]==49]\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}