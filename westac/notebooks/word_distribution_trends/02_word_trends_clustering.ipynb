{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/scipy/reference/cluster.vq.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Distribution Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "root_folder = os.path.abspath(os.path.join(globals()['_dh'][0], \"../../..\"))\n",
    "sys.path = [ root_folder ] + sys.path\n",
    "\n",
    "import ipywidgets\n",
    "import bokeh\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from beakerx import *\n",
    "from beakerx.object import beakerx\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import westac.common.goodness_of_fit as gof\n",
    "import westac.corpus.vectorized_corpus as vectorized_corpus\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "hv.extension('bokeh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previously vectorized corpus\n",
    "\n",
    "The corpus was created using the following settings:\n",
    " - Tokens were converted to lower case.\n",
    " - Only tokens that contains at least one alphanumeric character (isalnum).\n",
    " - Accents are ot removed (deacc)\n",
    " - Min token length 2 (min_len)\n",
    " - Max length not set (max_len)\n",
    " - Numerals are removed (numerals, -N)\n",
    " - Symbols are removed (symbols, -S)\n",
    "\n",
    "Use the `corpus_vectorizer` module to create a new corpus with different settings.\n",
    "\n",
    "The loaded corpus is processed in the following ways:\n",
    "\n",
    " - Exclude tokens having a total word count less than 10000\n",
    " - Include at most 50000 most frequent words words.\n",
    " - Group documents by year (y_corpus).\n",
    " - Normalize token count based on each year's total token count. \n",
    " - Normalize token distrubution over years to 1.0 (n_corpus).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_corpus = vectorized_corpus\\\n",
    "    .load_corpus('SOU_test_L0_+N_+S', os.path.join(root_folder, 'output'), n_count=5000, n_top=50000, axis=1, keep_magnitude=False)\n",
    "\n",
    "n_corpus = y_corpus.normalize(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deviation metrics\n",
    "\n",
    "These metrics are used to identify tokens that have a distribution that deviates the most to a uniform distribution.\n",
    "The following metrics are computed for each token:\n",
    "\n",
    "| Metric |  | |\n",
    "| :------ | :------- | :------ |\n",
    "| L2-norm | Measures distance to unform distribution. Lower bound 1/sqrt(d) is uniformity and upper bound is a 1-hot vector. |\n",
    "| Linear regression | Curve fitting to f(x) = k * x + m, where k is the slope, and m is the intercept to y axis when x is 0. A uniform curve has slope equals to 0. |\n",
    "| χ2 test | [Khan](https://www.khanacademy.org/math/statistics-probability/inference-categorical-data-chi-square-tests/chi-square-goodness-of-fit-tests/v/pearson-s-chi-square-test-goodness-of-fit) [Blog](https://towardsdatascience.com/inferential-statistic-understanding-hypothesis-testing-using-chi-square-test-eacf9fcac533) [Wikipedia](https://en.wikipedia.org/wiki/Chi-squared_test) |\n",
    "| Statistics | The min, max, mean and variance of the distribution |\n",
    "| Earth mover distance | [PDF](http://infolab.stanford.edu/pub/cstr/reports/cs/tr/99/1620/CS-TR-99-1620.ch4.pdf) [Wikipedia](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)  |\n",
    "| Kullback-Leibler divergence | S = sum(pk * log(pk / qk), where pk is the token distribution and qk is the uniform distribution |\n",
    "| Entropy | Basically the same as KLD |\n",
    "| Skew | A measure of the \"skewness\" of the token distribution. See [scipy.stats.skew](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skew.html#scipy.stats.skew) |\n",
    "\n",
    "References:\n",
    "\n",
    " - [StackExchange](stats.stackexchange.com/questions/25827/how-does-one-measure-the-non-uniformity-of-a-distribution)\n",
    "    \"It just so happens, though, that the L2 norm has a simple algebraic connection to the χ2 statistic used in goodness of fit tests:\n",
    "    that's the reason it might be suitable to measure non-uniformity\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def compute_uniformity_metrics(x_corpus, n_count=2000):\n",
    "\n",
    "    df_gof = gof.compute_goddness_of_fits_to_uniform(x_corpus)\n",
    "    df_most_deviating = gof.compile_most_deviating_words(df_gof, n_count=n_count)\n",
    "\n",
    "    return df_gof, df_most_deviating\n",
    "\n",
    "def display_uniformity_metrics(x_corpus, df_gof, df_most_deviating):\n",
    "    \n",
    "    output_row = [ ipywidgets.Output(), ipywidgets.Output() ]\n",
    "    display(ipywidgets.HBox(output_row))\n",
    "    \n",
    "    with output_row[0]:\n",
    "        \n",
    "        columns = ['token', 'word_count', 'l2_norm', 'slope', 'chi2_stats', 'earth_mover', 'kld', 'skew']\n",
    "        display(df_gof.nlargest(10000, columns=['word_count'])[columns])\n",
    "        \n",
    "    with output_row[1]:\n",
    "        pass\n",
    "        #columns = ['L2 token', 'L2', 'K token', 'K', 'X2 token', 'X2', 'EMD token', 'EMD', 'KLD token', 'KLD', 'Ent. token', 'Entropy']\n",
    "        #df_most_deviating.columns = columns\n",
    "        #display(df_most_deviating.head(100))\n",
    "    \n",
    "    gof.plot_metrics(df_gof)\n",
    "    gof.plot_slopes(x_corpus, df_most_deviating, 'l2_norm')\n",
    "\n",
    "    # df_gof.to_csv('df_gof.txt', sep='\\t')\n",
    "    # df_most_deviating.to_csv('df_most_deviating.txt', sep='\\t')\n",
    "\n",
    "\n",
    "df_gof, df_most_deviating = compute_uniformity_metrics(n_corpus, n_count=10000)\n",
    "\n",
    "display_uniformity_metrics(n_corpus, df_gof, df_most_deviating)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import westac.notebooks_gui.distributions_plot_gui as pdg\n",
    "\n",
    "tokens = df_most_deviating['l2_norm_token']\n",
    "pdg.display_gui(n_corpus, tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis (Hierarchical Clustering & K-means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import westac.notebooks_gui.cluster_analysis_gui as cluster_analysis_gui\n",
    "\n",
    "container = cluster_analysis_gui.display_gui(n_corpus, df_gof)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Some references\n",
    "\n",
    "|  |  |\n",
    "| :------- | :------- |\n",
    "| Overview: | https://docs.scipy.org/doc/scipy-0.19.0/reference/cluster.hierarchy.html |\n",
    "| Linkage: |https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage |\n",
    "| Reference | Daniel Mullner, “Modern hierarchical, agglomerative clustering algorithms”, [arXiv:1109.2378v1](https://arxiv.org/abs/1109.2378v1). |\n",
    "|  | [Link](https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/) [Link](https://github.com/giusdp/Clustering-Techniques/tree/0c78d1a893995c4603ed07216d645801ab4fdb4d)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
