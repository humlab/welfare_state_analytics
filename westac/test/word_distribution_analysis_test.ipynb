{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "sys.path = list(set(sys.path + ['../common']))\n",
    "\n",
    "import unittest\n",
    "import text_corpus\n",
    "import corpus_vectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utility\n",
    "import types\n",
    "import typing\n",
    "import matplotlib\n",
    "import nltk.tokenize\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [ x for ws in l for x in ws]\n",
    "\n",
    "class MockedProcessedCorpus():\n",
    "\n",
    "    def __init__(self, mock_data):\n",
    "        self.tokenized_documents = [ (f,y,self.generate_document(ws)) for f,y,ws in mock_data]\n",
    "        self.vocabulary = self.create_vocabulary()\n",
    "        self.n_tokens = { f: len(d) for f,y,d in mock_corpus_data }\n",
    "\n",
    "    def get_index(self):\n",
    "\n",
    "        return [\n",
    "            types.SimpleNamespace(filename=x[0],year=x[1]) for x in self.tokenized_documents\n",
    "        ]\n",
    "    \n",
    "    def create_vocabulary(self):\n",
    "        return { w: i for i, w in enumerate(sorted(list(set(flatten([ x[2] for x in self.tokenized_documents]))))) }\n",
    "        \n",
    "    def documents(self):\n",
    "\n",
    "        for filename, year, tokens in self.tokenized_documents:\n",
    "            yield types.SimpleNamespace(filename=filename, year=year), tokens\n",
    "\n",
    "    def generate_document(self, words):\n",
    "        if isinstance(words, str):\n",
    "            #parts = re.findall(r\"(\\d*)\\**(\\w+)\\S?\", words)\n",
    "            #words = [ (1 if x[0] == '' else int(x[0]), x[1]) for x in parts ]\n",
    "            document = words.split()\n",
    "        else:\n",
    "            document =  flatten([ n * w for n, w in words])\n",
    "        return document\n",
    "\n",
    "def mock_corpus():\n",
    "    mock_corpus_data = [\n",
    "        ('document_2013_1.txt', 2013, \"a a b c c c c d\"),\n",
    "        ('document_2013_2.txt', 2013, \"a a b b c c c\"),\n",
    "        ('document_2014_1.txt', 2014, \"a a b b b c c\"),\n",
    "        ('document_2014_2.txt', 2014, \"a a b b b b c d\"),\n",
    "        ('document_2014_2.txt', 2014, \"a a c d\")\n",
    "    ]\n",
    "    corpus = MockedProcessedCorpus(mock_corpus_data)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...............................\n",
      "----------------------------------------------------------------------\n",
      "Ran 31 tests in 0.319s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f844c238e80>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import utility\n",
    "\n",
    "class test_TextFilesReader(unittest.TestCase):\n",
    "    \n",
    "    def test_archive_filenames_when_filter_txt_returns_txt_files(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, pattern='*.txt')\n",
    "        self.assertEqual(5, len(reader.archive_filenames))\n",
    "\n",
    "    def test_archive_filenames_when_filter_md_returns_md_files(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, pattern='*.md')\n",
    "        self.assertEqual(1, len(reader.archive_filenames))\n",
    "\n",
    "    def test_archive_filenames_when_filter_function_txt_returns_txt_files(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        itemfilter = lambda _, x: x.endswith('txt')\n",
    "        reader = utility.TextFilesReader(filename, itemfilter=itemfilter)\n",
    "        self.assertEqual(5, len(reader.archive_filenames))\n",
    "\n",
    "    def test_get_file_when_default_returns_unmodified_content(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_01_test.txt'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=False, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Tre svarta ekar ur snön.\\r\\n\" + \\\n",
    "                   \"Så grova, men fingerfärdiga.\\r\\n\" + \\\n",
    "                   \"Ur deras väldiga flaskor\\r\\n\" + \\\n",
    "                   \"ska grönskan skumma i vår.\"\n",
    "        self.assertEqual(document_name, result[0])\n",
    "        self.assertEqual(expected, result[1])\n",
    "        \n",
    "    def test_can_get_file_when_compress_whitespace_is_true_strips_whitespaces(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_01_test.txt'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=True, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Tre svarta ekar ur snön. \" + \\\n",
    "                   \"Så grova, men fingerfärdiga. \" + \\\n",
    "                   \"Ur deras väldiga flaskor \" + \\\n",
    "                   \"ska grönskan skumma i vår.\"\n",
    "        self.assertEqual(document_name, result[0])\n",
    "        self.assertEqual(expected, result[1])\n",
    "\n",
    "    def test_get_file_when_dehyphen_is_trye_removes_hyphens(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_03_test.txt'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=True, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Nordlig storm. Det är den i den tid när rönnbärsklasar mognar. Vaken i mörkret hör man \" + \\\n",
    "                   \"stjärnbilderna stampa i sina spiltor \" + \\\n",
    "                   \"högt över trädet\"\n",
    "        self.assertEqual(document_name, result[0])\n",
    "        self.assertEqual(expected, result[1])\n",
    "        \n",
    "    def test_get_file_when_file_exists_and_extractor_specified_returns_content_and_metadat(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        document_name = 'dikt_2019_03_test.txt'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        result = next(reader.get_file(document_name))\n",
    "        expected = \"Nordlig storm. Det är den i den tid när rönnbärsklasar mognar. Vaken i mörkret hör man \" + \\\n",
    "                   \"stjärnbilderna stampa i sina spiltor \" + \\\n",
    "                   \"högt över trädet\"\n",
    "        self.assertEqual(document_name, result[0].filename)\n",
    "        self.assertEqual(2019, result[0].year)\n",
    "        self.assertEqual(3, result[0].serial_no)\n",
    "        self.assertEqual(expected, result[1])\n",
    "        \n",
    "    def test_get_index_when_extractor_passed_returns_metadata(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        result = reader.metadata\n",
    "        expected = [\n",
    "            types.SimpleNamespace(filename='dikt_2019_01_test.txt', serial_no=1, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_02_test.txt', serial_no=2, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_03_test.txt', serial_no=3, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2020_01_test.txt', serial_no=1, year=2020),\n",
    "            types.SimpleNamespace(filename='dikt_2020_02_test.txt', serial_no=2, year=2020)]\n",
    "        \n",
    "        self.assertEqual(len(expected), len(result))\n",
    "        for i in range(0,len(expected)):\n",
    "            self.assertEqual(expected[i], result[i])\n",
    "\n",
    "class test_Utilities(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        pass\n",
    " \n",
    "    def test_dehypen(self):\n",
    "\n",
    "        text = 'absdef\\n'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual(text, result)\n",
    "\n",
    "        text = 'abs-def\\n'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual(text, result)\n",
    "        \n",
    "        text = 'abs - def\\n'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual(text, result)\n",
    "        \n",
    "        text = 'abs-\\ndef'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual('absdef\\n', result)\n",
    "        \n",
    "        text = 'abs- \\r\\n def'\n",
    "        result = utility.dehyphen(text)\n",
    "        self.assertEqual('absdef\\n', result)\n",
    "    \n",
    "    def test_compress_whitespaces(self):\n",
    "        \n",
    "        text = 'absdef\\n'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('absdef', result)\n",
    "\n",
    "        text = ' absdef \\n'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual( 'absdef', result)\n",
    "        \n",
    "        text = 'abs  def'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('abs def', result)\n",
    "        \n",
    "        text = 'abs\\n def'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('abs def', result)\n",
    "        \n",
    "        text = 'abs- \\r\\n def'\n",
    "        result = utility.compress_whitespaces(text)\n",
    "        self.assertEqual('abs- def', result)\n",
    "        \n",
    "class Test_ExtractMeta(unittest.TestCase):\n",
    " \n",
    "    def test_extract_metadata_when_valid_regexp_returns_metadata_values(self):\n",
    "        filename = 'SOU 1957_5 Namn.txt'\n",
    "        meta = utility.extract_metadata(filename, year=r\".{4}(\\d{4})\\_.*\", serial_no=\".{8}\\_(\\d+).*\")\n",
    "        self.assertEqual(5, meta.serial_no)\n",
    "        self.assertEqual(1957, meta.year)\n",
    "\n",
    "    def test_extract_metadata_when_invalid_regexp_returns_none(self):\n",
    "        filename = 'xyz.txt'\n",
    "        meta = utility.extract_metadata(filename, value=r\".{4}(\\d{4})\\_.*\")\n",
    "        self.assertEqual(None, meta.value)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...............................\n",
      "----------------------------------------------------------------------\n",
      "Ran 31 tests in 0.325s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f844c25c208>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Test_CorpusTextStream(unittest.TestCase):\n",
    " \n",
    "    def test_next_document_when_new_corpus_returns_document(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, compress_whitespaces=True, dehyphen=True)\n",
    "        corpus = text_corpus.CorpusTextStream(reader)\n",
    "        result = next(corpus.documents())\n",
    "        expected = \"Tre svarta ekar ur snön. \" + \\\n",
    "                   \"Så grova, men fingerfärdiga. \" + \\\n",
    "                   \"Ur deras väldiga flaskor \" + \\\n",
    "                   \"ska grönskan skumma i vår.\"\n",
    "        self.assertEqual(expected, result[1])\n",
    "\n",
    "    def test_get_index_when_extract_passed_returns_metadata(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        corpus = text_corpus.CorpusTextStream(reader)\n",
    "        result = corpus.get_index()\n",
    "        expected = [\n",
    "            types.SimpleNamespace(filename='dikt_2019_01_test.txt', serial_no=1, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_02_test.txt', serial_no=2, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2019_03_test.txt', serial_no=3, year=2019),\n",
    "            types.SimpleNamespace(filename='dikt_2020_01_test.txt', serial_no=1, year=2020),\n",
    "            types.SimpleNamespace(filename='dikt_2020_02_test.txt', serial_no=2, year=2020)\n",
    "        ]\n",
    "        self.assertEqual(len(expected), len(result))\n",
    "        for i in range(0,len(expected)):\n",
    "            self.assertEqual(expected[i], result[i])\n",
    "            \n",
    "    def test_get_index_when_no_extract_passed_returns_none(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=None, compress_whitespaces=True, dehyphen=True)\n",
    "        corpus = text_corpus.CorpusTextStream(reader)\n",
    "        result = corpus.get_index()\n",
    "        self.assertIsNone(result)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...............................\n",
      "----------------------------------------------------------------------\n",
      "Ran 31 tests in 0.325s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f844c4b02e8>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test_CorpusTokenStream(unittest.TestCase):\n",
    "    \n",
    "    def create_reader(self, compress_whitespaces=True, dehyphen=True, meta_extract=None):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        #meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=compress_whitespaces, dehyphen=dehyphen)\n",
    "        return reader\n",
    "        \n",
    "    def test_next_document_when_token_corpus_returns_tokenized_document(self):\n",
    "        reader = reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=False)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\", \".\",\n",
    "                    \"Så\", \"grova\", \",\", \"men\", \"fingerfärdiga\", \".\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\", \".\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_isalnum_true_skips_deliminators(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=True)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"Så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_get_index_when_extract_passed_returns_expected_count(self):\n",
    "        reader = self.create_reader(meta_extract=dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\"))\n",
    "        corpus = text_corpus.CorpusTokenStream(reader)\n",
    "        result = corpus.get_index()\n",
    "        self.assertEqual(5, len(result))\n",
    "        \n",
    "    def test_n_tokens_when_exhausted_iterater_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=False)\n",
    "        r_n_tokens = {}\n",
    "        for filename, tokens in corpus.documents():\n",
    "            r_n_tokens[filename] = len(tokens)\n",
    "        n_tokens = corpus.n_tokens\n",
    "        expected = {\n",
    "            'dikt_2019_01_test.txt': 22,\n",
    "            'dikt_2019_02_test.txt': 16,\n",
    "            'dikt_2019_03_test.txt': 26,\n",
    "            'dikt_2020_01_test.txt': 45,\n",
    "            'dikt_2020_02_test.txt': 21\n",
    "        }\n",
    "        self.assertEqual(expected, n_tokens)\n",
    "        self.assertEqual(expected, r_n_tokens)\n",
    "\n",
    "    def test_n_tokens_when_exhausted_and_isalnum_is_true_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.CorpusTokenStream(reader, isalnum=True)\n",
    "        r_n_tokens = {}\n",
    "        for filename, tokens in corpus.documents():\n",
    "            r_n_tokens[filename] = len(tokens)\n",
    "        n_tokens = corpus.n_tokens\n",
    "        expected = {\n",
    "            'dikt_2019_01_test.txt': 18,\n",
    "            'dikt_2019_02_test.txt': 14,\n",
    "            'dikt_2019_03_test.txt': 24,\n",
    "            'dikt_2020_01_test.txt': 42,\n",
    "            'dikt_2020_02_test.txt': 18\n",
    "        }\n",
    "        self.assertEqual(expected, n_tokens)\n",
    "        self.assertEqual(expected, r_n_tokens)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...............................\n",
      "----------------------------------------------------------------------\n",
      "Ran 31 tests in 0.330s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f844c216358>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test_ProcessedCorpus(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        pass\n",
    "    \n",
    "    def create_reader(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        return reader\n",
    "    \n",
    "    def test_next_document_when_isalnum_is_true_returns_all_tokens(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=False, to_lower=False, deacc=False, min_len=1, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\", \".\",\n",
    "                    \"Så\", \"grova\", \",\", \"men\", \"fingerfärdiga\", \".\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\", \".\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_isalnum_true_skips_deliminators(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=False, deacc=False, min_len=1, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"Tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"Så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"Ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_to_lower_is_true_returns_all_lowercase(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=1, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"i\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_min_len_is_two_returns_single_char_words_filtered_out(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=2, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"så\", \"grova\", \"men\", \"fingerfärdiga\",\n",
    "                    \"ur\", \"deras\", \"väldiga\", \"flaskor\",\n",
    "                    \"ska\", \"grönskan\", \"skumma\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_next_document_when_max_len_is_six_returns_filter_out_longer_words(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=2, max_len=6, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        _, tokens = next(corpus.documents())\n",
    "        expected = [\"tre\", \"svarta\", \"ekar\", \"ur\", \"snön\",\n",
    "                    \"så\", \"grova\", \"men\", \n",
    "                    \"ur\", \"deras\", \n",
    "                    \"ska\", \"skumma\", \"vår\"]\n",
    "        self.assertEqual(expected, tokens)\n",
    "        \n",
    "    def test_get_index_when_extract_passed_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        kwargs = dict(isalnum=False, to_lower=False, deacc=False, min_len=2, max_len=None, numerals=True)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        result = corpus.get_index()\n",
    "        self.assertEqual(5, len(result))\n",
    "        \n",
    "    def test_n_tokens_when_exhausted_and_isalnum_min_len_two_returns_expected_count(self):\n",
    "        reader = self.create_reader()\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, isalnum=True, min_len=2)\n",
    "        r_tokens = {}\n",
    "        for filename, tokens in corpus.documents():\n",
    "            r_tokens[filename] = len(tokens)\n",
    "        n_tokens = corpus.n_raw_tokens\n",
    "        n_expected = {\n",
    "            'dikt_2019_01_test.txt': 18,\n",
    "            'dikt_2019_02_test.txt': 14,\n",
    "            'dikt_2019_03_test.txt': 24,\n",
    "            'dikt_2020_01_test.txt': 42,\n",
    "            'dikt_2020_02_test.txt': 18\n",
    "        }\n",
    "        p_tokens = corpus.n_tokens\n",
    "        p_expected = {\n",
    "            'dikt_2019_01_test.txt': 17,\n",
    "            'dikt_2019_02_test.txt': 13,\n",
    "            'dikt_2019_03_test.txt': 21,\n",
    "            'dikt_2020_01_test.txt': 42,\n",
    "            'dikt_2020_02_test.txt': 18\n",
    "        }\n",
    "        self.assertEqual(n_expected, n_tokens)\n",
    "        self.assertEqual(p_expected, p_tokens)\n",
    "        self.assertEqual(p_expected, r_tokens)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...................................\n",
      "----------------------------------------------------------------------\n",
      "Ran 35 tests in 0.334s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f844cc31358>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Test_CorpusVectorizer(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        pass\n",
    "    \n",
    "    def mock_vectorizer(self):\n",
    "        corpus = mock_corpus()\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        vectorizer.fit_transform(corpus)\n",
    "        return vectorizer\n",
    "    \n",
    "    def create_corpus(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=2, max_len=None, numerals=False)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        return corpus\n",
    "    \n",
    "    def test_fit_transform_(self):\n",
    "        corpus = self.create_corpus()\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        vectorizer.fit_transform(corpus)\n",
    "        results = vectorizer.vocabulary\n",
    "        expected = {'tre': 69, 'svarta': 62, 'ekar': 9, 'ur': 72, 'snön': 54, 'så': 65, 'grova': 17, 'men': 32, 'fingerfärdiga': 13, 'deras': 6, 'väldiga': 78, 'flaskor': 14, 'ska': 50, 'grönskan': 19, 'skumma': 53, 'vår': 79, 'på': 44, 'väg': 77, 'det': 7, 'långa': 29, 'mörkret': 36, 'envist': 11, 'skimrar': 51, 'mitt': 33, 'armbandsur': 2, 'med': 31, 'tidens': 67, 'fångna': 16, 'insekt': 25, 'nordlig': 38, 'storm': 61, 'är': 81, 'den': 5, 'tid': 66, 'när': 39, 'rönnbärsklasar': 45, 'mognar': 34, 'vaken': 74, 'hör': 24, 'man': 30, 'stjärnbilderna': 59, 'stampa': 58, 'sina': 48, 'spiltor': 57, 'högt': 23, 'över': 82, 'trädet': 70, 'jag': 26, 'ligger': 28, 'sängen': 64, 'armarna': 1, 'utbredda': 73, 'ett': 12, 'ankare': 0, 'som': 55, 'grävt': 18, 'ner': 37, 'sig': 47, 'ordentligt': 42, 'och': 40, 'håller': 22, 'kvar': 27, 'skuggan': 52, 'flyter': 15, 'där': 8, 'ovan': 43, 'stora': 60, 'okända': 41, 'en': 10, 'del': 4, 'av': 3, 'säkert': 63, 'viktigare': 76, 'än': 80, 'har': 20, 'sett': 46, 'mycket': 35, 'verkligheten': 75, 'tärt': 71, 'här': 21, 'sommaren': 56, 'till': 68, 'sist': 49}\n",
    "        self.assertEqual(expected, results)\n",
    "        \n",
    "    def test_fit_transform(self):\n",
    "        corpus = mock_corpus()\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        vectorizer.fit_transform(corpus)\n",
    "        expected_vocab = {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n",
    "        expected_dtm = [\n",
    "            [2, 1, 4, 1],\n",
    "            [2, 2, 3, 0],\n",
    "            [2, 3, 2, 0],\n",
    "            [2, 4, 1, 1],\n",
    "            [2, 0, 1, 1]\n",
    "        ]\n",
    "        expected_word_counts = {'a': 10, 'b': 10, 'c': 11, 'd': 3}\n",
    "        self.assertEqual(expected_vocab, vectorizer.vocabulary)\n",
    "        self.assertEqual(expected_word_counts, vectorizer.word_counts)\n",
    "        self.assertTrue((expected_dtm == vectorizer.X.toarray()).all())\n",
    "        \n",
    "    def test_collapse_to_year(self):\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        X = np.array([\n",
    "            [2, 1, 4, 1],\n",
    "            [2, 2, 3, 0],\n",
    "            [2, 3, 2, 0],\n",
    "            [2, 4, 1, 1],\n",
    "            [2, 0, 1, 1]\n",
    "        ])\n",
    "        df = pd.DataFrame({'year': [ 2013, 2013, 2014, 2014, 2014 ]})\n",
    "        Y = vectorizer.collapse_to_year(X, df)\n",
    "        expected_ytm = [\n",
    "            [4, 3, 7, 1],\n",
    "            [6, 7, 4, 2]\n",
    "        ]\n",
    "        self.assertTrue((expected_ytm == Y).all())\n",
    "        \n",
    "    def test_collapse_by_category(self):\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        X = np.array([\n",
    "            [2, 1, 4, 1],\n",
    "            [2, 2, 3, 0],\n",
    "            [2, 3, 2, 0],\n",
    "            [2, 4, 1, 1],\n",
    "            [2, 0, 1, 1]\n",
    "        ])\n",
    "        df = pd.DataFrame({'year': [ 2013, 2013, 2014, 2014, 2014 ]})\n",
    "        Y, imap = vectorizer.collapse_by_category('year', X, df)\n",
    "        expected_ytm = [\n",
    "            [4, 3, 7, 1],\n",
    "            [6, 7, 4, 2]\n",
    "        ]\n",
    "        self.assertTrue((expected_ytm == Y).all())\n",
    "        \n",
    "    def test_normalize(self):\n",
    "        vectorizer = self.mock_vectorizer()\n",
    "        X = np.array([\n",
    "            [4, 3, 7, 1],\n",
    "            [6, 7, 4, 2]\n",
    "        ], dtype=np.float)\n",
    "        Y = vectorizer.normalize(X)\n",
    "        E = np.array([\n",
    "            [4, 3, 7, 1],\n",
    "            [6, 7, 4, 2]\n",
    "        ]) / (np.array([[15,19]]).T)\n",
    "        self.assertTrue((E == Y).all())\n",
    "        \n",
    "    def test_tokens_above_threshold(self):\n",
    "        vectorizer = self.mock_vectorizer()\n",
    "        tokens = vectorizer.tokens_above_threshold(4)\n",
    "        expected_tokens = {'a': 10, 'b': 10, 'c': 11 }\n",
    "        self.assertEqual(expected_tokens, tokens)\n",
    "        \n",
    "    def test_token_ids_above_threshold(self):\n",
    "        vectorizer = self.mock_vectorizer()\n",
    "        ids = vectorizer.token_ids_above_threshold(4)\n",
    "        expected_ids = [\n",
    "            vectorizer.vocabulary['a'],\n",
    "            vectorizer.vocabulary['b'],\n",
    "            vectorizer.vocabulary['c']\n",
    "        ]\n",
    "        self.assertEqual(expected_ids, ids)\n",
    "        \n",
    "    def test_word_counts(self):\n",
    "        corpus = self.create_corpus()\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "        results = vectorizer.word_counts\n",
    "        expected = {\n",
    "            'tre': 1, 'svarta': 1, 'ekar': 1, 'ur': 2, 'snön': 1, 'så': 3, 'grova': 1, 'men': 2, 'fingerfärdiga': 1,\n",
    "            'deras': 1, 'väldiga': 2, 'flaskor': 1, 'ska': 1, 'grönskan': 1, 'skumma': 1, 'vår': 1, 'på': 3, 'väg': 1,\n",
    "            'det': 3, 'långa': 1, 'mörkret': 2, 'envist': 1, 'skimrar': 1, 'mitt': 1, 'armbandsur': 1, 'med': 2, 'tidens': 1,\n",
    "            'fångna': 1, 'insekt': 1, 'nordlig': 1, 'storm': 1, 'är': 5, 'den': 3, 'tid': 1, 'när': 1, 'rönnbärsklasar': 1,\n",
    "            'mognar': 1, 'vaken': 1, 'hör': 1, 'man': 2, 'stjärnbilderna': 1, 'stampa': 1, 'sina': 1, 'spiltor': 1,\n",
    "            'högt': 1, 'över': 1, 'trädet': 1, 'jag': 4, 'ligger': 1, 'sängen': 1, 'armarna': 1, 'utbredda': 1, 'ett': 1,\n",
    "            'ankare': 1, 'som': 4, 'grävt': 1, 'ner': 1, 'sig': 1, 'ordentligt': 1, 'och': 2, 'håller': 1, 'kvar': 1,\n",
    "            'skuggan': 1, 'flyter': 1, 'där': 1, 'ovan': 1, 'stora': 1, 'okända': 1, 'en': 2, 'del': 1, 'av': 1, 'säkert': 1,\n",
    "            'viktigare': 1, 'än': 1, 'har': 2, 'sett': 1, 'mycket': 2, 'verkligheten': 1, 'tärt': 1, 'här': 1, 'sommaren': 1,\n",
    "            'till': 1, 'sist': 1\n",
    "        }\n",
    "        self.assertEqual(expected, results)\n",
    "        \n",
    "    def test_dump_can_be_loaded(self):\n",
    "\n",
    "        # Arrange\n",
    "        corpus = self.create_corpus()\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "        # Act\n",
    "        vectorizer.dump('dump_test', folder='./output')\n",
    "\n",
    "        # Assert\n",
    "        data_filename =\"./output/dump_test_vectorizer_data.pickle\"\n",
    "        matrix_filename = \"./output/dump_test_vector_data.npy\"\n",
    "        \n",
    "        self.assertTrue(os.path.isfile(data_filename))\n",
    "        self.assertTrue(os.path.isfile(matrix_filename))\n",
    "        \n",
    "        # Act\n",
    "        loaded_vectorizer = corpus_vectorizer.CorpusVectorizer().load('dump_test', folder='./output')\n",
    "        \n",
    "        # Assert\n",
    "        self.assertEqual(vectorizer.word_counts, loaded_vectorizer.word_counts)\n",
    "        self.assertEqual(vectorizer.document_index.to_dict(), loaded_vectorizer.document_index.to_dict())\n",
    "        self.assertEqual(vectorizer.vocabulary, loaded_vectorizer.vocabulary)\n",
    "        #self.assertEqual(vectorizer.X, loaded_vectorizer.X)\n",
    "        self.assertIsNone(loaded_vectorizer.corpus)\n",
    "        #self.assertEqual(vectorizer.vectorizer, loaded_vectorizer.vectorizer)\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import text_corpus\n",
    "import corpus_vectorizer\n",
    "import os\n",
    "import sklearn\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Test_ChiSquare(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        pass\n",
    "    \n",
    "    def create_corpus(self):\n",
    "        filename = './test/test_data/test_corpus.zip'\n",
    "        meta_extract = dict(year=r\".{5}(\\d{4})\\_.*\", serial_no=\".{9}\\_(\\d+).*\")\n",
    "        reader = utility.TextFilesReader(filename, meta_extract=meta_extract, compress_whitespaces=True, dehyphen=True)\n",
    "        kwargs = dict(isalnum=True, to_lower=True, deacc=False, min_len=2, max_len=None, numerals=False)\n",
    "        corpus = text_corpus.ProcessedCorpus(reader, **kwargs)\n",
    "        return corpus\n",
    "    \n",
    "    def test_chisquare(self):\n",
    "        \n",
    "        corpus = self.create_corpus()\n",
    "        vectorizer = corpus_vectorizer.CorpusVectorizer()\n",
    "        vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        id2token = { i: w for w, i in vectorizer.vocabulary.items() }\n",
    "        \n",
    "        Y = vectorizer.collapse_to_year()\n",
    "        Yn = vectorizer.normalize(Y, axis=1, norm='l1') \n",
    "\n",
    "        indices = vectorizer.token_ids_above_threshold(1)\n",
    "        Ynw = Yn[:, indices]\n",
    "\n",
    "        X2 = scipy.stats.chisquare(Ynw, f_exp=None, ddof=0, axis=0)\n",
    "        \n",
    "        # Use X2 so select top 500 words... (highest Power-Power_divergenceResult)\n",
    "        # Ynw = largest_by_chisquare()\n",
    "        #print(Ynw)\n",
    "        \n",
    "        linked = linkage(Ynw.T, 'ward')\n",
    "        #print(linked)\n",
    "        \n",
    "        labels = [ id2token[x] for x in indices ]\n",
    "        \n",
    "        #plt.figure(figsize=(24, 16))\n",
    "        #dendrogram(linked, orientation='top', labels=labels, distance_sort='descending', show_leaf_counts=True)\n",
    "        #plt.show()\n",
    "        \n",
    "        results = None\n",
    "        expected = None\n",
    "        \n",
    "        self.assertEqual(expected, results)\n",
    "        \n",
    "\n",
    "\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dists(vectorizer):\n",
    "    df = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names()))\n",
    "    df['year'] = df.index + 45\n",
    "    df = df.set_index('year')\n",
    "    df['year'] =  pd.Series(df.index).apply(lambda x: documents[x][0])\n",
    "    df[['krig']].plot() #.loc[df[\"000\"]==49]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
